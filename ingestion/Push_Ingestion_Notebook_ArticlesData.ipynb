{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector search in Python (Azure AI Search)\n",
    "\n",
    "This code demonstrates how to use Azure AI Search by using the push API to insert vectors into your search index:\n",
    "\n",
    "+ Create an index schema\n",
    "+ Load the sample data from a local folder\n",
    "+ Embed the documents in-memory using Azure OpenAI's text-embedding-3-large model\n",
    "+ Index the vector and nonvector fields on Azure AI Search\n",
    "+ Run a series of vector and hybrid queries, including metadata filtering and hybrid (text + vectors) search. \n",
    "\n",
    "The code uses Azure OpenAI to generate embeddings for title and content fields. You'll need access to Azure OpenAI to run this demo.\n",
    "\n",
    "The code reads the `articles_1000.json` file, which contains the input data for which embeddings need to be generated.\n",
    "\n",
    "The output is a combination of human-readable text and embeddings that can be pushed into a search index.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ An Azure subscription, with [access to Azure OpenAI](https://aka.ms/oai/access). You must have the Azure OpenAI service name and an API key.\n",
    "\n",
    "+ A deployment of the text-embedding-3-large embedding model.\n",
    "\n",
    "+ Azure AI Search, any tier, but choose a service that has sufficient capacity for your vector index. We recommend Basic or higher. [Enable semantic ranking](https://learn.microsoft.com/azure/search/semantic-how-to-enable-disable) if you want to run the hybrid query with semantic ranking.\n",
    "\n",
    "We used Python 3.11, [Visual Studio Code with the Python extension](https://code.visualstudio.com/docs/python/python-tutorial), and the [Jupyter extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) to test this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Python virtual environment in Visual Studio Code\n",
    "\n",
    "1. Open the Command Palette (Ctrl+Shift+P).\n",
    "1. Search for **Python: Create Environment**.\n",
    "1. Select **Venv**.\n",
    "1. Select a Python interpreter. Choose 3.10 or later.\n",
    "\n",
    "It can take a minute to set up. If you run into problems, see [Python environments in VS Code](https://code.visualstudio.com/docs/python/environments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r azure-search-vector-python-sample-requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (1.93.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai pandas tqdm "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# The following variables from your .env file are used in this notebook\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\", \"\")) if len(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\", \"\")) > 0 else DefaultAzureCredential()\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX\", \"vectest\")\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\", \"\") if len(os.getenv(\"AZURE_OPENAI_KEY\", \"\")) > 0 else None\n",
    "azure_openai_embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-large\")\n",
    "azure_openai_embedding_dimensions = int(os.getenv(\"AZURE_OPENAI_EMBEDDING_DIMENSIONS\", 1024))\n",
    "embedding_model_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-large\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset as csv file \n",
    "df = pd.read_csv(r'C:\\Users\\lananoor\\OneDrive - Microsoft\\AI Agents\\FilteredQueryAgent\\ingestion\\medium_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://medium.datadriveninvestor.com/is-fasta...</td>\n",
       "      <td>Is FastAPI going to replace¬†Django?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>226</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://medium.datadriveninvestor.com/whats-th...</td>\n",
       "      <td>What‚Äôs the Best Way to Buy a Reliable Luxury¬†Car?</td>\n",
       "      <td>The full cost of ownership makes buying brand-...</td>\n",
       "      <td>186</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://medium.datadriveninvestor.com/credit-r...</td>\n",
       "      <td>Credit Risk Assessment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://medium.datadriveninvestor.com/cash-is-...</td>\n",
       "      <td>Cash is Trash or Cash is King? What¬¥s it gonna...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://medium.datadriveninvestor.com/how-to-b...</td>\n",
       "      <td>How to be Flipin‚Äô awesome for your¬†fans</td>\n",
       "      <td>Flipboard magazines capitalize on marketing vi...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                url  \\\n",
       "0   1  https://medium.datadriveninvestor.com/is-fasta...   \n",
       "1   2  https://medium.datadriveninvestor.com/whats-th...   \n",
       "2   3  https://medium.datadriveninvestor.com/credit-r...   \n",
       "3   4  https://medium.datadriveninvestor.com/cash-is-...   \n",
       "4   5  https://medium.datadriveninvestor.com/how-to-b...   \n",
       "\n",
       "                                               title  \\\n",
       "0                Is FastAPI going to replace¬†Django?   \n",
       "1  What‚Äôs the Best Way to Buy a Reliable Luxury¬†Car?   \n",
       "2                             Credit Risk Assessment   \n",
       "3  Cash is Trash or Cash is King? What¬¥s it gonna...   \n",
       "4            How to be Flipin‚Äô awesome for your¬†fans   \n",
       "\n",
       "                                            subtitle  claps  responses  \\\n",
       "0                                                NaN    226          4   \n",
       "1  The full cost of ownership makes buying brand-...    186          3   \n",
       "2                                                NaN     76          0   \n",
       "3                                                NaN    139          0   \n",
       "4  Flipboard magazines capitalize on marketing vi...     34          0   \n",
       "\n",
       "   reading_time           publication        date  \n",
       "0             4  Data Driven Investor  2020-05-24  \n",
       "1            10  Data Driven Investor  2020-05-24  \n",
       "2             7  Data Driven Investor  2020-05-24  \n",
       "3             6  Data Driven Investor  2020-05-24  \n",
       "4             5  Data Driven Investor  2020-05-24  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View data set \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11642"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any rows with null values\n",
    "df_no_nulls = df.dropna()\n",
    "\n",
    "# Select the first 1000 rows\n",
    "df_1000 = df_no_nulls.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://medium.datadriveninvestor.com/whats-th...</td>\n",
       "      <td>What‚Äôs the Best Way to Buy a Reliable Luxury¬†Car?</td>\n",
       "      <td>The full cost of ownership makes buying brand-...</td>\n",
       "      <td>186</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://medium.datadriveninvestor.com/how-to-b...</td>\n",
       "      <td>How to be Flipin‚Äô awesome for your¬†fans</td>\n",
       "      <td>Flipboard magazines capitalize on marketing vi...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://medium.datadriveninvestor.com/biometri...</td>\n",
       "      <td>Biometrics for Authentication Security and Pri...</td>\n",
       "      <td>Biometrics is a growing field,¬†and‚Ä¶</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.datadriveninvestor.com/what-mak...</td>\n",
       "      <td>What Makes Travel So Thrilling?</td>\n",
       "      <td>Photo by Rosa¬†Diaz</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>https://medium.datadriveninvestor.com/lessons-...</td>\n",
       "      <td>Lessons Learned From Doing All The Productive ...</td>\n",
       "      <td>As advised by¬†Google</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                                url  \\\n",
       "1    2  https://medium.datadriveninvestor.com/whats-th...   \n",
       "4    5  https://medium.datadriveninvestor.com/how-to-b...   \n",
       "6    7  https://medium.datadriveninvestor.com/biometri...   \n",
       "10  11  https://medium.datadriveninvestor.com/what-mak...   \n",
       "11  12  https://medium.datadriveninvestor.com/lessons-...   \n",
       "\n",
       "                                                title  \\\n",
       "1   What‚Äôs the Best Way to Buy a Reliable Luxury¬†Car?   \n",
       "4             How to be Flipin‚Äô awesome for your¬†fans   \n",
       "6   Biometrics for Authentication Security and Pri...   \n",
       "10                    What Makes Travel So Thrilling?   \n",
       "11  Lessons Learned From Doing All The Productive ...   \n",
       "\n",
       "                                             subtitle  claps  responses  \\\n",
       "1   The full cost of ownership makes buying brand-...    186          3   \n",
       "4   Flipboard magazines capitalize on marketing vi...     34          0   \n",
       "6                 Biometrics is a growing field,¬†and‚Ä¶    172          0   \n",
       "10                                 Photo by Rosa¬†Diaz     93          0   \n",
       "11                               As advised by¬†Google     87          0   \n",
       "\n",
       "    reading_time           publication        date  \n",
       "1             10  Data Driven Investor  2020-05-24  \n",
       "4              5  Data Driven Investor  2020-05-24  \n",
       "6              5  Data Driven Investor  2020-05-24  \n",
       "10             3  Data Driven Investor  2020-05-24  \n",
       "11             5  Data Driven Investor  2020-05-24  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1000.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new Content field using Azure OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is a city filled with iconic landmarks, world-class museums, charming neighborhoods, and delicious food. Here‚Äôs a list of must-see attractions and experiences to make the most of your trip:\n",
      "\n",
      "### **Iconic Landmarks**\n",
      "1. **Eiffel Tower**  \n",
      "   Visit the most recognizable landmark in Paris. You can admire it from the Trocad√©ro Gardens, picnic on the Champ de Mars, or go up to the top for stunning views of the city.\n",
      "\n",
      "2. **Louvre Museum**  \n",
      "   Home to masterpieces like the Mona Lisa and the Venus de Milo, the Louvre is a must-see for art lovers. Even if you don‚Äôt go inside, the glass pyramid is a sight to enjoy.\n",
      "\n",
      "3. **Notre-Dame Cathedral**  \n",
      "   Despite ongoing restoration after the 2019 fire, Notre-Dame remains a Gothic architectural masterpiece. Stroll around its exterior and explore √éle de la Cit√©.\n",
      "\n",
      "4. **Sacr√©-C≈ìur Basilica**  \n",
      "   Located at the highest point in Paris, Montmartre, this white basilica offers incredible views of the city.\n",
      "\n",
      "5. **Arc de Triomphe and Champs-√âlys√©es**  \n",
      "   Walk along the famous Champs-√âlys√©es and visit the Arc de Triomphe for panoramic views. Don‚Äôt forget that you can climb to the top!\n",
      "\n",
      "6. **Palace of Versailles** (Day Trip)  \n",
      "   Venture just outside Paris to this opulent palace and walk through the enchanting Hall of Mirrors and sprawling gardens.\n",
      "\n",
      "---\n",
      "\n",
      "### **World-Class Museums**\n",
      "7. **Mus√©e d'Orsay**  \n",
      "   Famous for its Impressionist and Post-Impressionist collections, including works by Monet, Van Gogh, and Renoir.\n",
      "\n",
      "8. **Centre Pompidou**  \n",
      "   A hub for modern and contemporary art with a rooftop view of Paris.\n",
      "\n",
      "9. **Mus√©e de l'Orangerie**  \n",
      "   Known for Monet‚Äôs Water Lilies murals, this museum is a peaceful escape.\n",
      "\n",
      "10. **Rodin Museum**  \n",
      "    Explore Rodin‚Äôs sculptures, such as *The Thinker*, in a romantic garden setting.\n",
      "\n",
      "---\n",
      "\n",
      "### **Charming Neighborhoods**\n",
      "11. **Montmartre**  \n",
      "    Wander the cobblestone streets of this bohemian district, visiting the Place du Tertre and the Moulin Rouge.\n",
      "\n",
      "12. **Le Marais**  \n",
      "    A trendy area with historic architecture, boutique shops, and delicious falafel (try L‚ÄôAs du Fallafel!).\n",
      "\n",
      "13. **Saint-Germain-des-Pr√©s**  \n",
      "    A chic district with famous caf√©s like Caf√© de Flore and Les Deux Magots. Explore art galleries and bookstores here.\n",
      "\n",
      "14. **Latin Quarter**  \n",
      "    A vibrant area filled with narrow streets, bookshops (including the famous Shakespeare & Company), and the Panth√©on.\n",
      "\n",
      "---\n",
      "\n",
      "### **Relaxing Spaces**\n",
      "15. **Jardin des Tuileries**  \n",
      "    A beautiful garden next to the Louvre, perfect for strolling or sitting.\n",
      "\n",
      "16. **Luxembourg Gardens**  \n",
      "    A tranquil spot to enjoy fountains, sculptures, and manicured lawns.\n",
      "\n",
      "17. **Parc des Buttes-Chaumont**  \n",
      "    A lesser-known park with a romantic vibe, featuring cliffs, waterfalls, and a view of Sacr√©-C≈ìur from a distance.\n",
      "\n",
      "---\n",
      "\n",
      "### **Historic Sites**\n",
      "18. **Conciergerie**  \n",
      "    Explore this medieval palace and prison where Marie Antoinette was held before her execution.\n",
      "\n",
      "19. **Pantheon**  \n",
      "    Honor legendary French figures such as Voltaire, Victor Hugo, and Marie Curie.\n",
      "\n",
      "---\n",
      "\n",
      "### **Food Experiences**\n",
      "20. **Parisian Caf√©s**  \n",
      "    Enjoy coffee and pastries at iconic caf√©s. Try a croissant at Angelina or a macaron at Ladur√©e.\n",
      "\n",
      "21. **Street Markets**  \n",
      "    Visit markets like March√© Bastille or Rue Cler for local produce, cheese, and bread.\n",
      "\n",
      "22. **Wine and Cheese Tasting**  \n",
      "    Indulge in French wines and artisanal cheeses.\n",
      "\n",
      "---\n",
      "\n",
      "### **Unique Experiences**\n",
      "23. **Seine River Cruise**  \n",
      "    See Paris from a different perspective on a boat tour that passes the Eiffel Tower, Notre-Dame, and other landmarks.\n",
      "\n",
      "24. **Catacombs of Paris**  \n",
      "    Venture underground to explore the ossuary that contains the remains of around six million people.\n",
      "\n",
      "25. **Opera Garnier**  \n",
      "    Marvel at this architectural gem and its lavish interiors.\n",
      "\n",
      "---\n",
      "\n",
      "### **Tips for Your Visit**\n",
      "- If you're visiting multiple attractions, consider purchasing a Paris Museum Pass for skip-the-line access.\n",
      "- Be prepared to walk a lot and wear comfortable shoes.\n",
      "- Don‚Äôt forget to take breaks to enjoy leisurely meals or coffee at the city's charming caf√©s.\n",
      "\n",
      "Enjoy your trip to Paris! Let me know if you'd like recommendations tailored to your specific interests. üòä\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Set up env variables \n",
    "endpoint = \"https://.openai.azure.com/\"\n",
    "model_name = \"gpt-4o\"\n",
    "deployment = \"gpt-4o\"\n",
    "\n",
    "subscription_key = \"\"\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I am going to Paris, what should I see?\",\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 119/1000 [03:00<28:23,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [24:45<00:00,  1.49s/it]\n",
      "C:\\Users\\lananoor\\AppData\\Local\\Temp\\ipykernel_44444\\3258349956.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_1000['content'] = contents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://medium.datadriveninvestor.com/whats-th...</td>\n",
       "      <td>What‚Äôs the Best Way to Buy a Reliable Luxury¬†Car?</td>\n",
       "      <td>The full cost of ownership makes buying brand-...</td>\n",
       "      <td>186</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>The article explores the optimal approach to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://medium.datadriveninvestor.com/how-to-b...</td>\n",
       "      <td>How to be Flipin‚Äô awesome for your¬†fans</td>\n",
       "      <td>Flipboard magazines capitalize on marketing vi...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>The article explores strategies for creating e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://medium.datadriveninvestor.com/biometri...</td>\n",
       "      <td>Biometrics for Authentication Security and Pri...</td>\n",
       "      <td>Biometrics is a growing field,¬†and‚Ä¶</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>The article explores the use of biometrics as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.datadriveninvestor.com/what-mak...</td>\n",
       "      <td>What Makes Travel So Thrilling?</td>\n",
       "      <td>Photo by Rosa¬†Diaz</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>The article, titled \"What Makes Travel So Thri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>https://medium.datadriveninvestor.com/lessons-...</td>\n",
       "      <td>Lessons Learned From Doing All The Productive ...</td>\n",
       "      <td>As advised by¬†Google</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>The article explores insights gained from impl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                                url  \\\n",
       "1    2  https://medium.datadriveninvestor.com/whats-th...   \n",
       "4    5  https://medium.datadriveninvestor.com/how-to-b...   \n",
       "6    7  https://medium.datadriveninvestor.com/biometri...   \n",
       "10  11  https://medium.datadriveninvestor.com/what-mak...   \n",
       "11  12  https://medium.datadriveninvestor.com/lessons-...   \n",
       "\n",
       "                                                title  \\\n",
       "1   What‚Äôs the Best Way to Buy a Reliable Luxury¬†Car?   \n",
       "4             How to be Flipin‚Äô awesome for your¬†fans   \n",
       "6   Biometrics for Authentication Security and Pri...   \n",
       "10                    What Makes Travel So Thrilling?   \n",
       "11  Lessons Learned From Doing All The Productive ...   \n",
       "\n",
       "                                             subtitle  claps  responses  \\\n",
       "1   The full cost of ownership makes buying brand-...    186          3   \n",
       "4   Flipboard magazines capitalize on marketing vi...     34          0   \n",
       "6                 Biometrics is a growing field,¬†and‚Ä¶    172          0   \n",
       "10                                 Photo by Rosa¬†Diaz     93          0   \n",
       "11                               As advised by¬†Google     87          0   \n",
       "\n",
       "    reading_time           publication        date  \\\n",
       "1             10  Data Driven Investor  2020-05-24   \n",
       "4              5  Data Driven Investor  2020-05-24   \n",
       "6              5  Data Driven Investor  2020-05-24   \n",
       "10             3  Data Driven Investor  2020-05-24   \n",
       "11             5  Data Driven Investor  2020-05-24   \n",
       "\n",
       "                                              content  \n",
       "1   The article explores the optimal approach to p...  \n",
       "4   The article explores strategies for creating e...  \n",
       "6   The article explores the use of biometrics as ...  \n",
       "10  The article, titled \"What Makes Travel So Thri...  \n",
       "11  The article explores insights gained from impl...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate summary for content field \n",
    "def generate_summary(title, subtitle):\n",
    "    prompt = (\n",
    "        f\"Given the following article information, generate a concise summary (about 200 words) of what the article is about. \"\n",
    "        f\"Base your summary ONLY on the title and subtitle. Do not add any extra information.\\n\\n\"\n",
    "        f\"Title: {title}\\nSubtitle: {subtitle}\\n\\nSummary:\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment,  # for Azure, use \"engine\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes articles based only on their title and subtitle.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=400,  # adjust if needed\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()  # <-- Corrected here\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Loop through the DataFrame and generate summaries\n",
    "contents = []\n",
    "for idx, row in tqdm(df_1000.iterrows(), total=len(df_1000)):\n",
    "    title = row['title']\n",
    "    subtitle = row['subtitle']\n",
    "    summary = generate_summary(title, subtitle)\n",
    "    contents.append(summary)\n",
    "\n",
    "# Add the new column\n",
    "df_1000['content'] = contents\n",
    "\n",
    "# Preview\n",
    "df_1000.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in 'content': 0\n"
     ]
    }
   ],
   "source": [
    "# Count the number of null values in the 'content' column\n",
    "null_count = df_1000['content'].isnull().sum()\n",
    "print(f\"Number of null values in 'content': {null_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert CSV to JSON fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to a list of dicts and write as JSON file\n",
    "df_1000.to_json(\"df_seperate_doc.json\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000.to_json(\"df_1000.json\", orient='records', lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries (one dict per row)\n",
    "records = df_1000.to_dict(orient='records')\n",
    "\n",
    "# Save as a JSON array (list of objects, one per row)\n",
    "with open(\"articles_1000.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id               int64\n",
      "url             object\n",
      "title           object\n",
      "subtitle        object\n",
      "claps            int64\n",
      "responses        int64\n",
      "reading_time     int64\n",
      "publication     object\n",
      "date            object\n",
      "content         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check Data Type \n",
    "print(df_1000.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add empty embedding field "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "Read your data, generate OpenAI embeddings and export to a format to insert your Azure AI Search index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Azure OpenAI config\n",
    "azure_openai_endpoint = \"https://.openai.azure.com/\"\n",
    "azure_openai_key = \"\"      \n",
    "azure_openai_embedding_deployment = \"text-embedding-3-large\"\n",
    "azure_openai_api_version = \"2023-05-15\"  \n",
    "embedding_model_name = azure_openai_embedding_deployment\n",
    "azure_openai_embedding_dimensions = 3072 \n",
    "\n",
    "client_embeddings = AzureOpenAI(\n",
    "    azure_deployment=azure_openai_embedding_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key\n",
    ")\n",
    "\n",
    "# Load your articles JSON (articles_1000.json)\n",
    "input_path = 'articles_1000.json'\n",
    "with open(input_path, 'r', encoding='utf-8') as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "# --- Prepare texts for embedding ---\n",
    "titles_text = [\n",
    "    (item.get('title', '') + '. ' + item.get('subtitle', '')).strip()\n",
    "    for item in input_data\n",
    "]\n",
    "content_text = [item.get('content', '') for item in input_data]\n",
    "\n",
    "# Optional: Replace empty strings with something (if you want to avoid blank embeddings)\n",
    "titles_text = [txt if txt.strip() else \"empty\" for txt in titles_text]\n",
    "content_text = [txt if txt.strip() else \"empty\" for txt in content_text]\n",
    "\n",
    "# --- Batch embedding function ---\n",
    "def batch(iterable, batch_size=16):\n",
    "    \"\"\"Yield successive batch_size-sized chunks from iterable.\"\"\"\n",
    "    for i in range(0, len(iterable), batch_size):\n",
    "        yield iterable[i:i + batch_size]\n",
    "\n",
    "# --- For titles_vector ---\n",
    "titles_embeddings = []\n",
    "for chunk in batch(titles_text, 16):\n",
    "    response = client_embeddings.embeddings.create(\n",
    "        input=chunk,\n",
    "        model=embedding_model_name,\n",
    "        dimensions=azure_openai_embedding_dimensions\n",
    "    )\n",
    "    titles_embeddings.extend([item.embedding for item in response.data])\n",
    "\n",
    "# --- For content_vector ---\n",
    "content_embeddings = []\n",
    "for chunk in batch(content_text, 16):\n",
    "    response = client_embeddings.embeddings.create(\n",
    "        input=chunk,\n",
    "        model=embedding_model_name,\n",
    "        dimensions=azure_openai_embedding_dimensions\n",
    "    )\n",
    "    content_embeddings.extend([item.embedding for item in response.data])\n",
    "\n",
    "# Assign embeddings to new fields in your documents\n",
    "for i, item in enumerate(input_data):\n",
    "    item['titlesVector'] = titles_embeddings[i]\n",
    "    item['contentVector'] = content_embeddings[i]\n",
    "\n",
    "# Output with new fields\n",
    "output_path = os.path.join('output', 'articles_final.json')\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(input_data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data Types in JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: {<class 'int'>}\n",
      "url: {<class 'str'>}\n",
      "title: {<class 'str'>}\n",
      "subtitle: {<class 'str'>}\n",
      "claps: {<class 'int'>}\n",
      "responses: {<class 'int'>}\n",
      "reading_time: {<class 'int'>}\n",
      "publication: {<class 'str'>}\n",
      "date: {<class 'str'>}\n",
      "content: {<class 'str'>}\n",
      "titlesVector: {<class 'list'>}\n",
      "contentVector: {<class 'list'>}\n"
     ]
    }
   ],
   "source": [
    "# Check data type of fields \n",
    "from collections import defaultdict\n",
    "\n",
    "types_per_field = defaultdict(set)\n",
    "\n",
    "for doc in data:\n",
    "    for k, v in doc.items():\n",
    "        types_per_field[k].add(type(v))\n",
    "\n",
    "for k, types in types_per_field.items():\n",
    "    print(f\"{k}: {types}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All IDs converted to strings.\n"
     ]
    }
   ],
   "source": [
    "# Convert id field from into to string \n",
    "import json\n",
    "\n",
    "# Load your JSON\n",
    "with open('output/articles_final.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert all 'id' fields to string\n",
    "for item in data:\n",
    "    item['id'] = str(item['id'])\n",
    "\n",
    "# Optionally, save it back\n",
    "with open('output/articles_final.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"All IDs converted to strings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 <class 'str'>\n",
      "2020-05-24 <class 'str'>\n",
      "2020-05-24 <class 'str'>\n",
      "2020-05-24 <class 'str'>\n",
      "2020-05-24 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Make sure date field is in the right format for DateTimeOffset in index \n",
    "\n",
    "with open('output/articles_final.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "for item in data[:5]:\n",
    "    print(item['date'], type(item['date']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: {<class 'str'>}\n",
      "url: {<class 'str'>}\n",
      "title: {<class 'str'>}\n",
      "subtitle: {<class 'str'>}\n",
      "claps: {<class 'int'>}\n",
      "responses: {<class 'int'>}\n",
      "reading_time: {<class 'int'>}\n",
      "publication: {<class 'str'>}\n",
      "date: {<class 'str'>}\n",
      "content: {<class 'str'>}\n",
      "titlesVector: {<class 'list'>}\n",
      "contentVector: {<class 'list'>}\n"
     ]
    }
   ],
   "source": [
    "# Final - Check data type of fields \n",
    "from collections import defaultdict\n",
    "\n",
    "types_per_field = defaultdict(set)\n",
    "\n",
    "for doc in data:\n",
    "    for k, v in doc.items():\n",
    "        types_per_field[k].add(type(v))\n",
    "\n",
    "for k, types in types_per_field.items():\n",
    "    print(f\"{k}: {types}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your search index\n",
    "\n",
    "Create your search index schema and vector search configuration. If you get an error, check the search service for available quota and check the .env file to make sure you're using a unique search index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://.search.windows.net\"\n",
    "api_key = \"\"\n",
    "credential = AzureKeyCredential(api_key)\n",
    "index_name = \"medium-articles-date-index\" # choose an index name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medium-articles-date-index created\n"
     ]
    }
   ],
   "source": [
    "index_name = \"medium-articles-date-index\"\n",
    "\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchFieldDataType,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters\n",
    ")\n",
    "\n",
    "# Set your actual endpoint, credential, and index_name\n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True, sortable=True, facetable=True),\n",
    "    SearchableField(name=\"url\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SearchableField(name=\"subtitle\", type=SearchFieldDataType.String),\n",
    "    SimpleField(name=\"claps\", type=SearchFieldDataType.Int32, filterable=True, sortable=True, facetable=True),\n",
    "    SimpleField(name=\"responses\", type=SearchFieldDataType.Int32, filterable=True, sortable=True, facetable=True),\n",
    "    SimpleField(name=\"reading_time\", type=SearchFieldDataType.Int32, filterable=True, sortable=True, facetable=True),\n",
    "    SearchableField(name=\"publication\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "    SimpleField(name=\"date\", type=SearchFieldDataType.DateTimeOffset, filterable=True, sortable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"titlesVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=3072, vector_search_profile_name=\"myHnswProfile\"),\n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=3072, vector_search_profile_name=\"myHnswProfile\"),\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "            vectorizer_name=\"myVectorizer\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            vectorizer_name=\"myVectorizer\",\n",
    "            parameters=AzureOpenAIVectorizerParameters(\n",
    "                resource_url=azure_openai_endpoint,\n",
    "                deployment_name=azure_openai_embedding_deployment,\n",
    "                model_name=embedding_model_name,\n",
    "                api_key=azure_openai_key\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        keywords_fields=[SemanticField(field_name=\"subtitle\"), SemanticField(field_name=\"publication\")],\n",
    "        content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search,\n",
    "    semantic_search=semantic_search\n",
    ")\n",
    "\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f'{result.name} created')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert text and embeddings into vector store\n",
    "Add texts and metadata from the JSON data to the vector store:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split JSON into smaller chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created articles_1.json with 10 docs.\n"
     ]
    }
   ],
   "source": [
    "# Split data into smaller batches \n",
    "import json\n",
    "\n",
    "input_path = r'C:\\Users\\lananoor\\OneDrive - Microsoft\\AI Agents\\FilteredQueryAgent\\ingestion\\output\\articles_final.json'\n",
    "output_path = r'C:\\Users\\lananoor\\OneDrive - Microsoft\\AI Agents\\FilteredQueryAgent\\ingestion\\output\\articles_1.json'\n",
    "\n",
    "# Read the full JSON file\n",
    "with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "    docs = json.load(infile)\n",
    "\n",
    "# Take the first 10 documents\n",
    "docs_10 = docs[:10]\n",
    "\n",
    "# Write them to the new file\n",
    "with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(docs_10, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Created articles_1.json with 10 docs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\\articles_1.json: 10 documents\n",
      "output\\articles_2.json: 90 documents\n",
      "output\\articles_3.json: 100 documents\n",
      "output\\articles_4.json: 200 documents\n",
      "output\\articles_5.json: 200 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for i in range(1, 6):\n",
    "    path = output_path = os.path.join('output', f'articles_{i}.json')\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            docs = json.load(f)\n",
    "            print(f\"{path}: {len(docs)} documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created articles_7.json with 200 docs.\n"
     ]
    }
   ],
   "source": [
    "# Split data into smaller batches \n",
    "import json\n",
    "\n",
    "input_path = r'C:\\Users\\lananoor\\OneDrive - Microsoft\\AI Agents\\FilteredQueryAgent\\ingestion\\output\\articles_final.json'\n",
    "output_path = r'C:\\Users\\lananoor\\OneDrive - Microsoft\\AI Agents\\FilteredQueryAgent\\ingestion\\output\\articles_7.json'\n",
    "\n",
    "# Read the full JSON file\n",
    "with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "    docs = json.load(infile)\n",
    "\n",
    "# Take the first 10 documents\n",
    "docs_10 = docs[800:1000]\n",
    "\n",
    "# Write them to the new file\n",
    "with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(docs_10, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Created articles_7.json with 200 docs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest JSON to AI Search Index in Batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "output_path = os.path.join('output', 'articles_1.json') \n",
    "\n",
    "# Upload some documents to the index  \n",
    "with open(output_path, 'r') as file:  \n",
    "    documents = json.load(file)  \n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)\n",
    "\n",
    "for res in result:\n",
    "    if not res.succeeded:\n",
    "        print(f\"Failed to upload doc id={res.key}, error: {res.error_message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 10 documents in total\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "\n",
    "output_path = os.path.join('output', 'articles_1.json') \n",
    "\n",
    "# Upload some documents to the index  \n",
    "with open(output_path, 'r', encoding='utf-8') as file:  \n",
    "    documents = json.load(file)  \n",
    "  \n",
    "# Use SearchIndexingBufferedSender to upload the documents in batches optimized for indexing  \n",
    "with SearchIndexingBufferedSender(  \n",
    "    endpoint=endpoint,  \n",
    "    index_name=index_name,  \n",
    "    credential=credential,  \n",
    ") as batch_client:  \n",
    "    # Add upload actions for all documents  \n",
    "    batch_client.upload_documents(documents=documents)  \n",
    "print(f\"Uploaded {len(documents)} documents in total\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 200 documents in total\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "\n",
    "output_path = os.path.join('output', 'articles_7.json') \n",
    "\n",
    "# Upload some documents to the index  \n",
    "with open(output_path, 'r', encoding='utf-8') as file:  \n",
    "    documents = json.load(file)  \n",
    "  \n",
    "# Use SearchIndexingBufferedSender to upload the documents in batches optimized for indexing  \n",
    "with SearchIndexingBufferedSender(  \n",
    "    endpoint=endpoint,  \n",
    "    index_name=index_name,  \n",
    "    credential=credential,  \n",
    ") as batch_client:  \n",
    "    # Add upload actions for all documents  \n",
    "    batch_client.upload_documents(documents=documents)  \n",
    "print(f\"Uploaded {len(documents)} documents in total\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Text Search  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 363\n",
      "Title: How to Overcome Your Phone Addiction With Mindfulness\n",
      "Publication: Better Humans\n",
      "Claps: 1300\n",
      "----------------------------------------\n",
      "ID: 497\n",
      "Title: How to Begin a Sugar-Free Life\n",
      "Publication: Better Humans\n",
      "Claps: 1200\n",
      "----------------------------------------\n",
      "ID: 743\n",
      "Title: Simple Changes That Helped Me Lose 5 Kg and Feel Great in Just One¬†Month\n",
      "Publication: Better Humans\n",
      "Claps: 1400\n",
      "----------------------------------------\n",
      "ID: 857\n",
      "Title: How to Select the Best Exercises for Building¬†Muscle\n",
      "Publication: Better Humans\n",
      "Claps: 523\n",
      "----------------------------------------\n",
      "ID: 974\n",
      "Title: Hardware to Boost Your Productivity in¬†2020\n",
      "Publication: Better Humans\n",
      "Claps: 2200\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Filter claps and publication \n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query using search.ismatch for Brodheadsville\n",
    "filter_query = \"publication eq 'Better Humans' and claps gt 500\"\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",  # Wildcard search to match all documents\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"publication\", \"claps\"],\n",
    "    top=5  # Limit to top 5 results\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Publication: {result['publication']}\")\n",
    "    print(f\"Claps: {result['claps']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 27\n",
      "Title: 10 eye-catching logo animations you‚Äôll wish you¬†made\n",
      "Publication: UX Collective\n",
      "Claps: 1700\n",
      "Responses: 10\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 30\n",
      "Title: How to stop the battle between Product Managers and Designers\n",
      "Publication: UX Collective\n",
      "Claps: 366\n",
      "Responses: 5\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 876\n",
      "Title: Loading: Neumorphism 2\n",
      "Publication: UX Collective\n",
      "Claps: 839\n",
      "Responses: 9\n",
      "Date: 2020-04-10T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 1202\n",
      "Title: 3D Design is in‚Ää‚Äî‚Äämy journey into trying 3D for the first¬†time\n",
      "Publication: UX Collective\n",
      "Claps: 292\n",
      "Responses: 7\n",
      "Date: 2020-02-21T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 1096\n",
      "Title: Change in Google Search is killing¬†it\n",
      "Publication: UX Collective\n",
      "Claps: 4700\n",
      "Responses: 46\n",
      "Date: 2020-02-16T00:00:00Z\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query\n",
    "filter_query = \"publication eq 'UX Collective' and responses ge 5\"\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",  # Wildcard search to match all documents\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"publication\", \"date\", \"claps\", \"responses\"],  # <-- fixed comma\n",
    "    top=5\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Publication: {result['publication']}\")\n",
    "    print(f\"Claps: {result['claps']}\")\n",
    "    print(f\"Responses: {result['responses']}\")\n",
    "    print(f\"Date: {result['date']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 27\n",
      "Title: 10 eye-catching logo animations you‚Äôll wish you¬†made\n",
      "Publication: UX Collective\n",
      "Responses: 10\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 30\n",
      "Title: How to stop the battle between Product Managers and Designers\n",
      "Publication: UX Collective\n",
      "Responses: 5\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Filter for publication, date and responses \n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query with correct DateTimeOffset literals\n",
    "filter_query = (\n",
    "    \"publication eq 'UX Collective' and \"\n",
    "    \"date ge 2020-05-01T00:00:00Z and \"\n",
    "    \"date le 2020-05-31T00:00:00Z and \"\n",
    "    \"responses ge 5\"\n",
    ")\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"publication\", \"date\", \"responses\"],\n",
    "    top=5\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Publication: {result['publication']}\")\n",
    "    print(f\"Responses: {result['responses']}\")\n",
    "    print(f\"Date: {result['date']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 389\n",
      "Title: I Lost $40,000 in a Month and Learned a Valuable¬†Lesson\n",
      "Publication: The Startup\n",
      "Responses: 30\n",
      "Date: 2020-01-03T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 625\n",
      "Title: Stop Checking for¬†Nulls\n",
      "Publication: The Startup\n",
      "Responses: 29\n",
      "Date: 2020-05-27T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 1208\n",
      "Title: Your Brain Is Not an Indestructible Punching¬†Bag\n",
      "Publication: The Startup\n",
      "Responses: 26\n",
      "Date: 2020-02-21T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 386\n",
      "Title: Biggest Startup Failure in¬†History\n",
      "Publication: The Startup\n",
      "Responses: 15\n",
      "Date: 2020-01-03T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 905\n",
      "Title: How to Fold The Deck Chairs on The¬†Titanic\n",
      "Publication: The Startup\n",
      "Responses: 11\n",
      "Date: 2020-04-10T00:00:00Z\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Filter for publication, date and responses \n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query with correct DateTimeOffset literals\n",
    "filter_query = (\n",
    "    \"publication eq 'The Startup' and \"\n",
    "    \"date ge 2019-05-01T00:00:00Z and \"\n",
    "    \"date le 2021-05-31T00:00:00Z and \"\n",
    "    \"responses ge 10\"\n",
    ")\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"publication\", \"date\", \"responses\"],\n",
    "    top=5\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Publication: {result['publication']}\")\n",
    "    print(f\"Responses: {result['responses']}\")\n",
    "    print(f\"Date: {result['date']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1670\n",
      "Title: Google Algorithm Update What You Need to¬†Know\n",
      "Subtitle: Remaining up-to-date with‚Ä¶\n",
      "Publication: Data Driven Investor\n",
      "Reading Time: 7\n",
      "----------------------------------------\n",
      "ID: 114\n",
      "Title: How to Set Up and Track Goals in Google Analytics\n",
      "Subtitle: Understanding if your marketing and content¬†is‚Ä¶\n",
      "Publication: The Startup\n",
      "Reading Time: 6\n",
      "----------------------------------------\n",
      "ID: 984\n",
      "Title: How to A/B Test With Google Optimize: A Complete¬†Guide\n",
      "Subtitle: Setup, install, test, and analyze multiple‚Ä¶\n",
      "Publication: Better Marketing\n",
      "Reading Time: 12\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query\n",
    "filter_query = \"search.ismatch('Google', 'title') and reading_time gt 5\"\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",  # Wildcard search to match all documents\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"subtitle\", \"publication\", \"reading_time\"],  # <-- fixed comma\n",
    "    top=5\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Subtitle: {result['subtitle']}\")\n",
    "    print(f\"Publication: {result['publication']}\")\n",
    "    print(f\"Reading Time: {result['reading_time']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 158\n",
      "Title: Who Rules the Cloud Service: AWS or¬†Azure?\n",
      "Subtitle: Comparison Between Amazon Web Service Vs Microsoft Azure\n",
      "Content: The article explores the competition between Amazon Web Services (AWS) and Microsoft Azure, two leading cloud service providers. It presents a comparison of the features, capabilities, and offerings of each platform to determine which dominates the cloud service industry.\n",
      "Reading Time: 10\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query\n",
    "filter_query = \"search.ismatch('Microsoft', 'content') and reading_time gt 5\"\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",  # Wildcard search to match all documents\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"subtitle\", \"content\", \"reading_time\"],  # <-- fixed comma\n",
    "    top=5\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Subtitle: {result['subtitle']}\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(f\"Reading Time: {result['reading_time']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 230\n",
      "Title: Introduction to Azure Cache for Redis with¬†.NET¬†Core\n",
      "Subtitle: Azure Cache for Redis provides us with a powerful‚Ä¶\n",
      "Content: The article titled \"Introduction to Azure Cache for Redis with .NET Core\" explores the capabilities of Azure Cache for Redis and its integration with .NET Core applications. The subtitle suggests that Azure Cache for Redis is a powerful tool, likely emphasizing its utility in enhancing application performance and scalability. The article likely provides an overview of how developers can leverage this caching service within their .NET Core projects to optimize data retrieval and processing.\n",
      "Reading Time: 9\n",
      "Responses: 0\n",
      "Claps: 96\n",
      "----------------------------------------\n",
      "ID: 158\n",
      "Title: Who Rules the Cloud Service: AWS or¬†Azure?\n",
      "Subtitle: Comparison Between Amazon Web Service Vs Microsoft Azure\n",
      "Content: The article explores the competition between Amazon Web Services (AWS) and Microsoft Azure, two leading cloud service providers. It presents a comparison of the features, capabilities, and offerings of each platform to determine which dominates the cloud service industry.\n",
      "Reading Time: 10\n",
      "Responses: 0\n",
      "Claps: 60\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Example filter:\n",
    "# - \"Azure\" appears in content\n",
    "# - reading_time more than 7\n",
    "# - responses more than 3\n",
    "# - claps at least 500\n",
    "\n",
    "filter_query = (\n",
    "    \"search.ismatch('Azure', 'content') \"\n",
    "    \"and reading_time gt 7 \"\n",
    "    \"and responses lt 3 \"\n",
    "    \"and claps lt 500\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",  # Wildcard search\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"subtitle\", \"content\", \"reading_time\", \"responses\", \"claps\"],  # typo fixed\n",
    "    top=5\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Subtitle: {result['subtitle']}\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(f\"Reading Time: {result['reading_time']}\")\n",
    "    print(f\"Responses: {result['responses']}\")\n",
    "    print(f\"Claps: {result['claps']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 519\n",
      "Title: Job titles in UX: the witch, the lion, and the¬†wardrobe\n",
      "Publication: UX Collective\n",
      "Claps: 17\n",
      "Date: 2020-09-05T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 754\n",
      "Title: Stop doing design system¬†projects\n",
      "Publication: UX Collective\n",
      "Claps: 737\n",
      "Date: 2020-09-20T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 756\n",
      "Title: A UX writing crash¬†course\n",
      "Publication: UX Collective\n",
      "Claps: 256\n",
      "Date: 2020-09-20T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 759\n",
      "Title: Every designer needs to know: Visual Attention\n",
      "Publication: UX Collective\n",
      "Claps: 105\n",
      "Date: 2020-09-20T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 761\n",
      "Title: iOS 14 is missing one important privacy¬†feature\n",
      "Publication: UX Collective\n",
      "Claps: 298\n",
      "Date: 2020-09-20T00:00:00Z\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Filter claps and publication \n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query using search.ismatch for Brodheadsville\n",
    "filter_query = \"publication eq 'UX Collective' and date ge 2020-06-01T00:00:00Z and date lt 2020-12-31T00:00:00Z\"\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",  # Wildcard search to match all documents\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"publication\", \"claps\", \"date\"],\n",
    "    top=5  # Limit to top 5 results\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Publication: {result['publication']}\")\n",
    "    print(f\"Claps: {result['claps']}\")\n",
    "    print(f\"Date: {result['date']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid + Semantic Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper code to print results\n",
    "\n",
    "from azure.search.documents import SearchItemPaged\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "def print_results(results: SearchItemPaged[dict]):\n",
    "    semantic_answers = results.get_answers()\n",
    "    if semantic_answers:\n",
    "        for answer in semantic_answers:\n",
    "            if answer.highlights:\n",
    "                print(f\"Semantic Answer: {answer.highlights}\")\n",
    "            else:\n",
    "                print(f\"Semantic Answer: {answer.text}\")\n",
    "            print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")  \n",
    "        print(f\"Score: {result['@search.score']}\")\n",
    "        if result.get('@search.reranker_score'):\n",
    "            print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "        print(f\"Subtitle: {result['subtitle']}\")      \n",
    "        print(f\"Content: {result['content']}\")   \n",
    "        print(f\"Claps: {result['claps']}\")\n",
    "        print(f\"Responses: {result['responses']}\")\n",
    "        print(f\"Date: {result['date']}\")\n",
    "        print(f\"Publication: {result['publication']}\\n\")\n",
    "\n",
    "        captions = result[\"@search.captions\"]\n",
    "        if captions:\n",
    "            caption = captions[0]\n",
    "            if caption.highlights:\n",
    "                print(f\"Caption: {caption.highlights}\\n\")\n",
    "            else:\n",
    "                print(f\"Caption: {caption.text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How to Increase Your Attention Span\n",
      "Score: 0.05000000447034836\n",
      "Subtitle: Strategies for deep work that go beyond putting your phone¬†in‚Ä¶\n",
      "Content: The article \"How to Increase Your Attention Span\" explores effective strategies for enhancing focus and productivity. It emphasizes techniques for engaging in deep work that extend beyond basic measures, such as simply putting your phone away.\n",
      "Claps: 3900\n",
      "Responses: 19\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "Publication: Better Humans\n",
      "\n",
      "Title: Attention Management\n",
      "Score: 0.048659902065992355\n",
      "Subtitle: For productivity and a meaningful life, stop trying to manage your time and start managing your attention.\n",
      "Content: The article \"Attention Management\" advocates shifting focus from traditional time management techniques to attention management as a more effective approach for enhancing productivity and leading a meaningful life. It suggests that managing where and how you direct your attention is key to achieving better outcomes, rather than simply trying to organize and optimize your schedule.\n",
      "Claps: 161\n",
      "Responses: 0\n",
      "Date: 2020-02-21T00:00:00Z\n",
      "Publication: The Startup\n",
      "\n",
      "Title: Every designer needs to know: Visual Attention\n",
      "Score: 0.04569460451602936\n",
      "Subtitle: Have you ever failed to remember an actor's face?¬†or‚Ä¶\n",
      "Content: The article explores the concept of visual attention and its importance for designers. It delves into how individuals perceive and process visual information, including the challenges of recognizing and recalling faces or other visual details. By understanding the mechanisms of visual attention, designers can create more effective and memorable designs that align with how people naturally interact with visual stimuli.\n",
      "Claps: 105\n",
      "Responses: 0\n",
      "Date: 2020-09-20T00:00:00Z\n",
      "Publication: UX Collective\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Multi Vector Search\n",
    "query = \"tips for improving attention span\"  \n",
    "  \n",
    "vector_query_1 = VectorizableTextQuery(text=query, k_nearest_neighbors=50, fields=\"contentVector\")\n",
    "vector_query_2 = VectorizableTextQuery(text=query, k_nearest_neighbors=50, fields=\"titlesVector\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query_1, vector_query_2],\n",
    "    select=[\"title\", \"subtitle\", \"content\", \"publication\", \"responses\", \"claps\", \"date\"],\n",
    "    top=3\n",
    ")  \n",
    "  \n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How to Increase Your Attention Span\n",
      "Score: 0.038333334028720856\n",
      "Subtitle: Strategies for deep work that go beyond putting your phone¬†in‚Ä¶\n",
      "Content: The article \"How to Increase Your Attention Span\" explores effective strategies for enhancing focus and productivity. It emphasizes techniques for engaging in deep work that extend beyond basic measures, such as simply putting your phone away.\n",
      "Claps: 3900\n",
      "Responses: 19\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "Publication: Better Humans\n",
      "\n",
      "Title: Attention Management\n",
      "Score: 0.037184491753578186\n",
      "Subtitle: For productivity and a meaningful life, stop trying to manage your time and start managing your attention.\n",
      "Content: The article \"Attention Management\" advocates shifting focus from traditional time management techniques to attention management as a more effective approach for enhancing productivity and leading a meaningful life. It suggests that managing where and how you direct your attention is key to achieving better outcomes, rather than simply trying to organize and optimize your schedule.\n",
      "Claps: 161\n",
      "Responses: 0\n",
      "Date: 2020-02-21T00:00:00Z\n",
      "Publication: The Startup\n",
      "\n",
      "Title: Every designer needs to know: Visual Attention\n",
      "Score: 0.035246841609478\n",
      "Subtitle: Have you ever failed to remember an actor's face?¬†or‚Ä¶\n",
      "Content: The article explores the concept of visual attention and its importance for designers. It delves into how individuals perceive and process visual information, including the challenges of recognizing and recalling faces or other visual details. By understanding the mechanisms of visual attention, designers can create more effective and memorable designs that align with how people naturally interact with visual stimuli.\n",
      "Claps: 105\n",
      "Responses: 0\n",
      "Date: 2020-09-20T00:00:00Z\n",
      "Publication: UX Collective\n",
      "\n",
      "Title: 8 Tips That Will Change Your Writing in 3¬†Minutes\n",
      "Score: 0.03189147636294365\n",
      "Subtitle: Writing, editing, money, rejections, and¬†more\n",
      "Content: The article provides eight actionable tips designed to quickly improve your writing skills in just three minutes. It covers various aspects of the writing process, including crafting and refining your work, handling rejections, and navigating financial aspects related to writing. The guidance aims to enhance both the creative and practical sides of writing, offering advice that is concise and impactful.\n",
      "Claps: 1800\n",
      "Responses: 15\n",
      "Date: 2020-09-03T00:00:00Z\n",
      "Publication: Better Marketing\n",
      "\n",
      "Title: <strong class=\"markup--strong markup--h3-strong\"><em class=\"markup--em markup--h3-em\">My 4 Self-Improvement Tips for¬†2020</em></strong>\n",
      "Score: 0.0313466377556324\n",
      "Subtitle: <strong class=\"markup--strong markup--h4-strong\"><em class=\"markup--em markup--h4-em\">A former 12-year (two-time NBA micro-failing, MVP-winning)</em></strong>\n",
      "Content: The article titled \"My 4 Self-Improvement Tips for 2020\" provides personal insights and advice for self-improvement, specifically tailored for the year 2020. Written by someone with a unique background as a former two-time NBA participant who experienced setbacks (\"micro-failings\") and achieved success as an MVP winner, the author draws from their experiences to share four actionable tips. The subtitle highlights the author's journey of overcoming challenges and leveraging lessons from both failure and success, suggesting the self-improvement advice is grounded in resilience and personal growth.\n",
      "Claps: 88\n",
      "Responses: 1\n",
      "Date: 2020-01-03T00:00:00Z\n",
      "Publication: Data Driven Investor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Weighted Multi Vector Hybrid Search\n",
    "query = \"tips for improving attention span\"  \n",
    "  \n",
    "vector_query_1 = VectorizableTextQuery(text=query, k_nearest_neighbors=50, fields=\"contentVector\", weight=1.0)\n",
    "vector_query_2 = VectorizableTextQuery(text=query, k_nearest_neighbors=50, fields=\"titlesVector\", weight=0.3)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query_1, vector_query_2],\n",
    "    select=[\"title\", \"subtitle\", \"content\", \"publication\", \"responses\", \"claps\", \"date\"],\n",
    "    top=5\n",
    ")  \n",
    "  \n",
    "print_results(results)\n",
    "  \n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Hybrid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Introduction to Azure Cache for Redis with¬†.NET¬†Core\n",
      "Score: 0.032786883413791656\n",
      "Reranker Score: 2.0227746963500977\n",
      "Subtitle: Azure Cache for Redis provides us with a powerful‚Ä¶\n",
      "Content: The article titled \"Introduction to Azure Cache for Redis with .NET Core\" explores the capabilities of Azure Cache for Redis and its integration with .NET Core applications. The subtitle suggests that Azure Cache for Redis is a powerful tool, likely emphasizing its utility in enhancing application performance and scalability. The article likely provides an overview of how developers can leverage this caching service within their .NET Core projects to optimize data retrieval and processing.\n",
      "Claps: 96\n",
      "Responses: 0\n",
      "Date: 2020-12-29T00:00:00Z\n",
      "Publication: The Startup\n",
      "\n",
      "Caption: The article titled \"Introduction to<em> Azure Cache for Redis with </em>.NET Core\" explores the capabilities of<em> Azure Cache for Redis </em>and its integration with .NET Core applications. The subtitle suggests that<em> Azure Cache for Redis </em>is a<em> powerful tool, </em>likely emphasizing its utility in enhancing application performance and scalability. The article likely.\n",
      "\n",
      "Title: Who Rules the Cloud Service: AWS or¬†Azure?\n",
      "Score: 0.03279569745063782\n",
      "Reranker Score: 2.0051398277282715\n",
      "Subtitle: Comparison Between Amazon Web Service Vs Microsoft Azure\n",
      "Content: The article explores the competition between Amazon Web Services (AWS) and Microsoft Azure, two leading cloud service providers. It presents a comparison of the features, capabilities, and offerings of each platform to determine which dominates the cloud service industry.\n",
      "Claps: 60\n",
      "Responses: 0\n",
      "Date: 2020-12-29T00:00:00Z\n",
      "Publication: Data Driven Investor\n",
      "\n",
      "Caption: The article explores the competition between Amazon Web Services (AWS) and Microsoft Azure, two leading cloud service providers. It presents a comparison of the features, capabilities, and offerings of each platform to determine which dominates the cloud service industry.\n",
      "\n",
      "Title: From Product to¬†Platform\n",
      "Score: 0.029571646824479103\n",
      "Reranker Score: 1.8227654695510864\n",
      "Subtitle: Why I believe that \n",
      "Content: The article \"From Product to Platform\" explores a shift in focus from individual products to broader platforms. The subtitle, \"Why I believe that,\" suggests the author provides personal insights or reasoning behind the importance of this transition. It likely discusses the advantages and implications of adopting a platform-based approach in comparison to a traditional product-centric strategy.\n",
      "Claps: 125\n",
      "Responses: 0\n",
      "Date: 2020-06-19T00:00:00Z\n",
      "Publication: Data Driven Investor\n",
      "\n",
      "Caption: The article \"From Product to Platform\" explores a shift in focus from<em> individual products </em>to<em> broader platforms.</em> The subtitle, \"Why I believe that,\" suggests the author provides<em> personal insights </em>or reasoning behind the importance of this transition. It likely discusses the advantages and implications of adopting a platform-based approach in.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType\n",
    "\n",
    "# Semantic Hybrid Search\n",
    "query = \"what are products from Azure?\"\n",
    "\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=50, fields=\"contentVector\", exhaustive=True)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"subtitle\", \"content\", \"publication\", \"responses\", \"claps\", \"date\"], \n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=3\n",
    ")\n",
    "\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Answer: The article titled \"How You Can Become Fluent in a Language‚Ää‚Äî‚ÄäIn Just One Year\" with the subtitle \"A complete plan for language acquisition\" outlines a comprehensive strategy for achieving fluency in a... <em>It likely provides a structured approach to mastering language skills, emphasizing practical methods and actionable steps for efficient learning.</em>\n",
      "Semantic Answer Score: 0.9449999928474426\n",
      "\n",
      "Title: How You Can Become Fluent in a Language‚Ää‚Äî‚ÄäIn Just One¬†Year\n",
      "Score: 0.03333333507180214\n",
      "Reranker Score: 3.4150407314300537\n",
      "Subtitle: A complete plan for language acquisition‚Ä¶\n",
      "Content: The article titled \"How You Can Become Fluent in a Language‚Ää‚Äî‚ÄäIn Just One Year\" with the subtitle \"A complete plan for language acquisition\" outlines a comprehensive strategy for achieving fluency in a new language within a year. It likely provides a structured approach to mastering language skills, emphasizing practical methods and actionable steps for efficient learning. The focus appears to be on creating a clear, achievable plan that guides readers through the process of language acquisition in a relatively short time frame.\n",
      "Claps: 4000\n",
      "Responses: 40\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "Publication: Better Humans\n",
      "\n",
      "Caption: The article titled \"How You Can Become Fluent in a Language‚Ää‚Äî‚ÄäIn Just One Year\" with the subtitle \"A complete plan for language acquisition\" outlines a comprehensive strategy for achieving fluency in a new language within a year. It likely provides a structured approach to mastering language skills, emphasizing practical methods and actionable.\n",
      "\n",
      "Title: How to Write Better; Improve Your Vocabulary With 5 Free¬†Tools\n",
      "Score: 0.022722331807017326\n",
      "Reranker Score: 1.897912859916687\n",
      "Subtitle: If you are not a linguist or English¬†is‚Ä¶\n",
      "Content: The article provides guidance on improving writing skills by enhancing vocabulary. It introduces five free tools designed to help users expand their word knowledge, making writing more effective and engaging. The article targets individuals who may not have a background in linguistics or for whom English is not their first language, offering accessible resources to support their language development.\n",
      "Claps: 805\n",
      "Responses: 7\n",
      "Date: 2020-02-21T00:00:00Z\n",
      "Publication: The Startup\n",
      "\n",
      "Caption: The article provides guidance on<em> improving writing skills by enhancing vocabulary.</em> It introduces<em> five free tools designed to help users expand their word knowledge, making writing more effective and engaging.</em> The article targets<em> individuals who may not have a background in linguistics or for whom English is not their first language, </em>offering.\n",
      "\n",
      "Title: Writing for¬†Design\n",
      "Score: 0.02418670430779457\n",
      "Reranker Score: 1.7617967128753662\n",
      "Subtitle: Creating user experience through¬†language\n",
      "Content: The article \"Writing for Design: Creating user experience through language\" explores the role of language in shaping user experiences within design contexts. It emphasizes how thoughtful and intentional writing can enhance the usability and functionality of a design, ensuring that users interact seamlessly with products or interfaces. By focusing on the integration of language as a key design element, the article likely discusses strategies for crafting clear, engaging, and user-centric content that complements visual and structural design components.\n",
      "Claps: 279\n",
      "Responses: 2\n",
      "Date: 2020-01-11T00:00:00Z\n",
      "Publication: UX Collective\n",
      "\n",
      "Caption: The article \"Writing for Design: Creating user experience through language\" explores the<em> role of language </em>in shaping user experiences within design contexts. It emphasizes how<em> thoughtful and intentional writing can enhance the usability and functionality of a </em>design, ensuring that us... By focusing on the<em> integration of language </em>as a key design.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType\n",
    "\n",
    "# Semantic Hybrid Search\n",
    "query = \"How do become fluent in a language?\"\n",
    "\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=50, fields=\"contentVector\", exhaustive=True)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"subtitle\", \"content\", \"publication\", \"responses\", \"claps\", \"date\"], \n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=3\n",
    ")\n",
    "\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Hybrid Search with Query Rewriting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Answer: The article, \"Welcome to Python, Meet the Dunders Pt. 3,\" continues a series focused on exploring Python's dunder methods (special methods with double underscores). In this third installment, the author delves deeper into these methods,<em> shedding light </em>on<em> their functionality and usage within the Python programming language.</em>\n",
      "Semantic Answer Score: 0.8899999856948853\n",
      "\n",
      "Title: How You Can Become Fluent in a Language‚Ää‚Äî‚ÄäIn Just One¬†Year\n",
      "Score: 0.03333333507180214\n",
      "Reranker Score: 2.7216992378234863\n",
      "Subtitle: A complete plan for language acquisition‚Ä¶\n",
      "Content: The article titled \"How You Can Become Fluent in a Language‚Ää‚Äî‚ÄäIn Just One Year\" with the subtitle \"A complete plan for language acquisition\" outlines a comprehensive strategy for achieving fluency in a new language within a year. It likely provides a structured approach to mastering language skills, emphasizing practical methods and actionable steps for efficient learning. The focus appears to be on creating a clear, achievable plan that guides readers through the process of language acquisition in a relatively short time frame.\n",
      "Claps: 4000\n",
      "Responses: 40\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "Publication: Better Humans\n",
      "\n",
      "Caption: The article titled \"How You Can Become Fluent in a Language‚Ää‚Äî‚ÄäIn Just One Year\" with the subtitle \"A complete plan for language acquisition\" outlines a comprehensive strategy for achieving fluency in a new language within a year. It likely provides a structured approach to mastering language skills, emphasizing practical methods and actionable.\n",
      "\n",
      "Title: How to Write Better; Improve Your Vocabulary With 5 Free¬†Tools\n",
      "Score: 0.03177805617451668\n",
      "Reranker Score: 1.9432849884033203\n",
      "Subtitle: If you are not a linguist or English¬†is‚Ä¶\n",
      "Content: The article provides guidance on improving writing skills by enhancing vocabulary. It introduces five free tools designed to help users expand their word knowledge, making writing more effective and engaging. The article targets individuals who may not have a background in linguistics or for whom English is not their first language, offering accessible resources to support their language development.\n",
      "Claps: 805\n",
      "Responses: 7\n",
      "Date: 2020-02-21T00:00:00Z\n",
      "Publication: The Startup\n",
      "\n",
      "Caption: The article provides guidance on<em> improving writing skills </em>by<em> enhancing vocabulary.</em> It introduces five free tools designed to help users expand their word knowledge, making writing more effective and engaging. The article targets individuals who may not have a background in linguistics or for whom English is not their first language, offering.\n",
      "\n",
      "Title: Writing for¬†Design\n",
      "Score: 0.0317460335791111\n",
      "Reranker Score: 1.8755813837051392\n",
      "Subtitle: Creating user experience through¬†language\n",
      "Content: The article \"Writing for Design: Creating user experience through language\" explores the role of language in shaping user experiences within design contexts. It emphasizes how thoughtful and intentional writing can enhance the usability and functionality of a design, ensuring that users interact seamlessly with products or interfaces. By focusing on the integration of language as a key design element, the article likely discusses strategies for crafting clear, engaging, and user-centric content that complements visual and structural design components.\n",
      "Claps: 279\n",
      "Responses: 2\n",
      "Date: 2020-01-11T00:00:00Z\n",
      "Publication: UX Collective\n",
      "\n",
      "Caption: The article \"Writing for Design: Creating user experience through<em> language\" </em>explores the<em> role of language </em>in shaping user experiences within design contexts. It emphasizes<em> how thoughtful and intentional writing </em>can<em> enhance the usability and functionality of a design, </em>ensuring that users interact seamlessly with products or interfaces. By focusing.\n",
      "\n",
      "Text Query Rewrites:\n",
      "['how to improve language fluency', 'improving fluency in languages', 'improving fluency in a language', 'improving language fluency', 'improving fluency in different languages']\n",
      "Vector Query Rewrites:\n",
      "['how to improve language fluency', 'improving fluency in languages', 'improving fluency in a language', 'improving language fluency', 'improving fluency in different languages']\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType, QueryDebugMode\n",
    "from typing import Optional\n",
    "\n",
    "# Workaround required to use debug query rewrites with the preview SDK\n",
    "import azure.search.documents._generated.models\n",
    "azure.search.documents._generated.models.SearchDocumentsResult._attribute_map[\"debug_info\"][\"key\"] = \"@search\\\\.debug\"\n",
    "from azure.search.documents._generated.models import DebugInfo\n",
    "import azure.search.documents._paging\n",
    "def get_debug_info(self) -> Optional[DebugInfo]:\n",
    "    self.continuation_token = None\n",
    "    return self._response.debug_info\n",
    "azure.search.documents._paging.SearchPageIterator.get_debug_info = azure.search.documents._paging._ensure_response(get_debug_info)\n",
    "azure.search.documents._paging.SearchItemPaged.get_debug_info = lambda self: self._first_iterator_instance().get_debug_info()\n",
    "\n",
    "# Semantic Hybrid Search with Query Rewriting\n",
    "query = \"language fluency\"\n",
    "\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=50, fields=\"contentVector\", query_rewrites=\"generative|count-5\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"subtitle\", \"content\", \"publication\", \"responses\", \"claps\", \"date\"], \n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    query_rewrites=\"generative|count-5\",\n",
    "    query_language=\"en\",\n",
    "    debug=QueryDebugMode.QUERY_REWRITES,\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=3\n",
    ")\n",
    "\n",
    "text_query_rewrites = results.get_debug_info().query_rewrites.text.rewrites\n",
    "vector_query_rewrites = results.get_debug_info().query_rewrites.vectors[0].rewrites\n",
    "print_results(results)\n",
    "\n",
    "print(\"Text Query Rewrites:\")\n",
    "print(text_query_rewrites)\n",
    "print(\"Vector Query Rewrites:\")\n",
    "print(vector_query_rewrites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Answer: The article explores<em> strategies for bridging the divide between digital and physical experiences.</em> It emphasizes the importance of addressing this gap, which is a significant focus of ongoing discussions.\n",
      "Semantic Answer Score: 0.8989999890327454\n",
      "\n",
      "Title: Data liberation pattern using Debezium¬†engine\n",
      "Score: 0.025676939636468887\n",
      "Reranker Score: 1.8759357929229736\n",
      "Subtitle: Integrating legacy applications into your Event-Driven‚Ä¶\n",
      "Content: The article \"Data liberation pattern using Debezium engine\" discusses a method for integrating legacy applications into event-driven architectures. It focuses on utilizing the Debezium engine to implement a data liberation pattern, which likely involves extracting and streaming data from older systems to facilitate seamless integration with modern event-driven workflows.\n",
      "Claps: 92\n",
      "Responses: 0\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "Publication: The Startup\n",
      "\n",
      "Caption: The article \"Data<em> liberation </em>pattern using<em> Debezium engine\" </em>discusses a method for<em> integrating legacy applications into event-driven architectures.</em> It focuses on<em> utilizing the Debezium engine </em>to<em> implement a data liberation pattern, </em>which likely involves<em> extracting and streaming data from older systems </em>to facilitate seamless integration with modern.\n",
      "\n",
      "Title: What are Dark¬†Patterns\n",
      "Score: 0.014784879982471466\n",
      "Reranker Score: 1.7582520246505737\n",
      "Subtitle: What red lines should you never cross to get the attention of your users,¬†or‚Ä¶\n",
      "Content: The article explores the concept of \"Dark Patterns,\" which are deceptive design strategies used to manipulate users' behavior or decisions online. It delves into the ethical boundaries that should not be crossed when attempting to capture users' attention. The focus appears to be on identifying these unethical practices and advocating for responsible design that respects user autonomy and trust.\n",
      "Claps: 26\n",
      "Responses: 0\n",
      "Date: 2020-09-20T00:00:00Z\n",
      "Publication: UX Collective\n",
      "\n",
      "Caption: The article explores the concept of<em> \"Dark Patterns,\" </em>which are<em> deceptive design strategies </em>used to<em> manipulate users' </em>behavior<em> or </em>decisions<em> online.</em> It delves into the ethical boundaries that should not be crossed when attempting to capture users' attention. The focus appears to be on identifying these unethical practices and advocating for.\n",
      "\n",
      "Title: Creating Simplicity Through Abstraction\n",
      "Score: 0.03201844170689583\n",
      "Reranker Score: 1.7376928329467773\n",
      "Subtitle: Enforcing Conceptual Structures in System¬†Design\n",
      "Content: The article \"Creating Simplicity Through Abstraction: Enforcing Conceptual Structures in System Design\" explores the role of abstraction in simplifying complex systems. It emphasizes the importance of establishing clear conceptual structures to enhance system design, making it more manageable and efficient. By leveraging abstraction, the article likely discusses strategies to reduce unnecessary complexity, streamline processes, and enforce organized frameworks that guide the development and functionality of systems.\n",
      "Claps: 118\n",
      "Responses: 0\n",
      "Date: 2020-04-30T00:00:00Z\n",
      "Publication: The Startup\n",
      "\n",
      "Caption: The article \"Creating Simplicity Through Abstraction: Enforcing<em> Conceptual </em>Structures in<em> System Design\" </em>explores the role of<em> abstraction </em>in<em> simplifying complex systems.</em> It emphasizes the importance of<em> establishing clear conceptual structures to enhance system design, </em>making it more manageable and efficient.  ...straction, the article likely.\n",
      "\n",
      "Text Query Rewrites:\n",
      "['DESI systems architecture design', 'architectures of digital systems', 'DESI systems architecture', 'types of systems architectures', 'systems architecture design principles']\n",
      "Vector Query Rewrites:\n",
      "['DESI systems architecture design', 'architectures of digital systems', 'DESI systems architecture', 'types of systems architectures', 'systems architecture design principles']\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType, QueryDebugMode\n",
    "from typing import Optional\n",
    "\n",
    "# Workaround required to use debug query rewrites with the preview SDK\n",
    "import azure.search.documents._generated.models\n",
    "azure.search.documents._generated.models.SearchDocumentsResult._attribute_map[\"debug_info\"][\"key\"] = \"@search\\\\.debug\"\n",
    "from azure.search.documents._generated.models import DebugInfo\n",
    "import azure.search.documents._paging\n",
    "def get_debug_info(self) -> Optional[DebugInfo]:\n",
    "    self.continuation_token = None\n",
    "    return self._response.debug_info\n",
    "azure.search.documents._paging.SearchPageIterator.get_debug_info = azure.search.documents._paging._ensure_response(get_debug_info)\n",
    "azure.search.documents._paging.SearchItemPaged.get_debug_info = lambda self: self._first_iterator_instance().get_debug_info()\n",
    "\n",
    "# Semantic Hybrid Search with Query Rewriting\n",
    "query = \"desifn systems architectures\"\n",
    "\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=50, fields=\"contentVector\", query_rewrites=\"generative|count-5\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"subtitle\", \"content\", \"publication\", \"responses\", \"claps\", \"date\"], \n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    query_rewrites=\"generative|count-5\",\n",
    "    query_language=\"en\",\n",
    "    debug=QueryDebugMode.QUERY_REWRITES,\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=3\n",
    ")\n",
    "\n",
    "text_query_rewrites = results.get_debug_info().query_rewrites.text.rewrites\n",
    "vector_query_rewrites = results.get_debug_info().query_rewrites.vectors[0].rewrites\n",
    "print_results(results)\n",
    "\n",
    "print(\"Text Query Rewrites:\")\n",
    "print(text_query_rewrites)\n",
    "print(\"Vector Query Rewrites:\")\n",
    "print(vector_query_rewrites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Answer: The article \"Sacred Spaces: Making Space For Creative Work\" explores the concept of creating dedicated environments that<em> nurture creativity and productivity.</em> It emphasizes the importance of carving out intentional spaces that inspire and support creative endeavors, suggesting that such spaces can serve as sanctuaries for focused work and artisti...\n",
      "Semantic Answer Score: 0.9589999914169312\n",
      "\n",
      "Title: The Complete Guide to Music for Productivity\n",
      "Score: 0.03333333507180214\n",
      "Reranker Score: 3.0981454849243164\n",
      "Subtitle: Get into flow by using science to choose the right¬†tunes‚Ä¶\n",
      "Content: The article, \"The Complete Guide to Music for Productivity,\" explores how music can be used as a tool to enhance focus and efficiency. It emphasizes leveraging scientific insights to select the most suitable tunes for achieving a state of flow, where optimal productivity occurs.\n",
      "Claps: 614\n",
      "Responses: 6\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "Publication: Better Humans\n",
      "\n",
      "Caption: The article, \"The Complete<em> Guide </em>to<em> Music for Productivity,\" </em>explores<em> how music can be used as a tool to enhance focus and efficiency.</em> It emphasizes<em> leveraging scientific insights to select the most suitable tunes for achieving a state of flow, where optimal productivity occurs.</em>\n",
      "\n",
      "Title: Sacred Spaces\n",
      "Score: 0.019055943936109543\n",
      "Reranker Score: 2.1230452060699463\n",
      "Subtitle: Making Space For Creative¬†Work\n",
      "Content: The article \"Sacred Spaces: Making Space For Creative Work\" explores the concept of creating dedicated environments that nurture creativity and productivity. It emphasizes the importance of carving out intentional spaces that inspire and support creative endeavors, suggesting that such spaces can serve as sanctuaries for focused work and artistic expression.\n",
      "Claps: 323\n",
      "Responses: 3\n",
      "Date: 2020-02-16T00:00:00Z\n",
      "Publication: The Startup\n",
      "\n",
      "Caption: The article \"Sacred Spaces: Making Space For Creative Work\" explores the concept of creating dedicated environments that<em> nurture creativity and productivity.</em> It emphasizes the importance of carving out intentional spaces that inspire and support creative endeavors, suggesting that such spaces can serve as sanctuaries for focused work and artistic.\n",
      "\n",
      "Title: Rest is Productive\n",
      "Score: 0.026690078899264336\n",
      "Reranker Score: 1.9000396728515625\n",
      "Subtitle: How to use time off to boost your creativity and become more efficient\n",
      "Content: The article \"Rest is Productive\" explores the concept of using time off as a tool to enhance creativity and efficiency. It emphasizes the importance of rest as a productive activity, suggesting that taking breaks or stepping away from work can lead to improved performance and innovative thinking. By reframing rest as an essential component of productivity, the article likely provides insights or strategies on how individuals can optimize their downtime to recharge and maximize their creative potential.\n",
      "Claps: 795\n",
      "Responses: 8\n",
      "Date: 2020-05-27T00:00:00Z\n",
      "Publication: Better Humans\n",
      "\n",
      "Caption: The article<em> \"Rest </em>is<em> Productive\" </em>explores the concept of using time off as a tool to enhance creativity and efficiency. It emphasizes the importance of<em> rest as a productive activity, </em>suggesting that taking breaks or stepping away from work can lead to improved performance and innovative thinking. By<em> reframing rest </em>as an essential component of.\n",
      "\n",
      "Text Query Rewrites:\n",
      "['using music to increase productivity', 'how music improves focus and productivity', 'impact of music on work productivity', 'music as a tool for productivity', 'using music to improve focus and productivity']\n",
      "Vector Query Rewrites:\n",
      "['using music to increase productivity', 'how music improves focus and productivity', 'impact of music on work productivity', 'music as a tool for productivity', 'using music to improve focus and productivity']\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType, QueryDebugMode\n",
    "from typing import Optional\n",
    "\n",
    "# Workaround required to use debug query rewrites with the preview SDK\n",
    "import azure.search.documents._generated.models\n",
    "azure.search.documents._generated.models.SearchDocumentsResult._attribute_map[\"debug_info\"][\"key\"] = \"@search\\\\.debug\"\n",
    "from azure.search.documents._generated.models import DebugInfo\n",
    "import azure.search.documents._paging\n",
    "def get_debug_info(self) -> Optional[DebugInfo]:\n",
    "    self.continuation_token = None\n",
    "    return self._response.debug_info\n",
    "azure.search.documents._paging.SearchPageIterator.get_debug_info = azure.search.documents._paging._ensure_response(get_debug_info)\n",
    "azure.search.documents._paging.SearchItemPaged.get_debug_info = lambda self: self._first_iterator_instance().get_debug_info()\n",
    "\n",
    "# Semantic Hybrid Search with Query Rewriting\n",
    "query = \"music and productivity\"\n",
    "\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=50, fields=\"contentVector\", query_rewrites=\"generative|count-5\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"subtitle\", \"content\", \"publication\", \"responses\", \"claps\", \"date\"], \n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    query_rewrites=\"generative|count-5\",\n",
    "    query_language=\"en\",\n",
    "    debug=QueryDebugMode.QUERY_REWRITES,\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=3\n",
    ")\n",
    "\n",
    "text_query_rewrites = results.get_debug_info().query_rewrites.text.rewrites\n",
    "vector_query_rewrites = results.get_debug_info().query_rewrites.vectors[0].rewrites\n",
    "print_results(results)\n",
    "\n",
    "print(\"Text Query Rewrites:\")\n",
    "print(text_query_rewrites)\n",
    "print(\"Vector Query Rewrites:\")\n",
    "print(vector_query_rewrites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Answer: The article discusses the author's experience using reverse dieting to restore healthy eating habits. <em>Reverse dieting, a gradual increase in calorie intake, </em>helped the author<em> transition from consuming a restrictive 1,200 calories per day to a more sustainable and balanced </em>approach<em> to nutrition.</em> The piece likely explores the challenges of restric...\n",
      "Semantic Answer Score: 0.9300000071525574\n",
      "\n",
      "Title: How I Used Reverse Dieting to Recover Healthy Eating¬†Habits\n",
      "Score: 0.03229166567325592\n",
      "Reranker Score: 2.6791629791259766\n",
      "Subtitle: I went from eating 1,200 calories a day¬†to‚Ä¶\n",
      "Content: The article discusses the author's experience using reverse dieting to restore healthy eating habits. Reverse dieting, a gradual increase in calorie intake, helped the author transition from consuming a restrictive 1,200 calories per day to a more sustainable and balanced approach to nutrition. The piece likely explores the challenges of restrictive eating and how reverse dieting supported recovery and improved overall well-being.\n",
      "Claps: 601\n",
      "Responses: 2\n",
      "Date: 2020-09-20T00:00:00Z\n",
      "Publication: Better Humans\n",
      "\n",
      "Caption: The article discusses the author's experience using reverse dieting to restore healthy eating habits. <em>Reverse dieting, a gradual increase in calorie intake, </em>helped the author<em> transition from consuming a restrictive 1,200 calories per day to a more sustainable and balanced </em>approach<em> to nutrition.</em> The piece likely explores the challenges of.\n",
      "\n",
      "Title: How I Used Reverse Dieting to Recover Healthy Eating¬†Habits\n",
      "Score: 0.032522473484277725\n",
      "Reranker Score: 2.6437158584594727\n",
      "Subtitle: I went from eating 1,200 calories a day¬†to‚Ä¶\n",
      "Content: The article discusses the author's experience with reverse dieting as a method to restore healthy eating habits. It highlights the journey of transitioning from a restrictive diet of consuming only 1,200 calories per day to adopting a more balanced and sustainable approach to nutrition. Reverse dieting is presented as a key strategy used by the author to gradually increase caloric intake while improving overall dietary patterns.\n",
      "Claps: 601\n",
      "Responses: 2\n",
      "Date: 2020-09-05T00:00:00Z\n",
      "Publication: Better Humans\n",
      "\n",
      "Caption: The article discusses the author's experience with reverse dieting as a method to restore healthy eating habits. It highlights the journey of<em> transitioning from a restrictive diet of consuming only 1,200 calories per day </em>to<em> adopting </em>a<em> more balanced </em>and<em> sustainable </em>approach to<em> nutrition.</em> Reverse dieting is presented as a key strategy used by the.\n",
      "\n",
      "Title: How I Stopped Binge Eating and Developed a Healthy Relationship with¬†Food\n",
      "Score: 0.03279569745063782\n",
      "Reranker Score: 2.579202651977539\n",
      "Subtitle: Action steps you can take¬†to‚Ä¶\n",
      "Content: The article titled \"How I Stopped Binge Eating and Developed a Healthy Relationship with Food\" with the subtitle \"Action steps you can take to‚Ä¶\" likely discusses the author's personal journey in overcoming binge eating and establishing a healthier mindset and habits around food. It emphasizes actionable strategies that readers can implement to address similar challenges and build a balanced relationship with eating. The focus appears to be on practical guidance and steps for fostering positive changes in food-related behaviors.\n",
      "Claps: 597\n",
      "Responses: 9\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "Publication: Better Humans\n",
      "\n",
      "Caption: The article titled \"How I Stopped<em> Binge </em>Eating and<em> Developed a Healthy Relationship with Food\" </em>with the subtitle \"Action steps you can take to\". likely discusses the author's<em> personal </em>journey in<em> overcoming binge eating </em>and<em> establishing a healthier mindset and habits around food.</em> It<em> emphasizes actionable strategies that </em>readers can implement to.\n",
      "\n",
      "Text Query Rewrites:\n",
      "['how to eat healthily', 'best practices for a healthy diet', 'tips for a healthy diet', 'how to eat a healthy diet', 'best eating habits for health']\n",
      "Vector Query Rewrites:\n",
      "['how to eat healthily', 'best practices for a healthy diet', 'tips for a healthy diet', 'how to eat a healthy diet', 'best eating habits for health']\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType, QueryDebugMode\n",
    "from typing import Optional\n",
    "\n",
    "# Workaround required to use debug query rewrites with the preview SDK\n",
    "import azure.search.documents._generated.models\n",
    "azure.search.documents._generated.models.SearchDocumentsResult._attribute_map[\"debug_info\"][\"key\"] = \"@search\\\\.debug\"\n",
    "from azure.search.documents._generated.models import DebugInfo\n",
    "import azure.search.documents._paging\n",
    "def get_debug_info(self) -> Optional[DebugInfo]:\n",
    "    self.continuation_token = None\n",
    "    return self._response.debug_info\n",
    "azure.search.documents._paging.SearchPageIterator.get_debug_info = azure.search.documents._paging._ensure_response(get_debug_info)\n",
    "azure.search.documents._paging.SearchItemPaged.get_debug_info = lambda self: self._first_iterator_instance().get_debug_info()\n",
    "\n",
    "# Semantic Hybrid Search with Query Rewriting\n",
    "query = \"healthy eating\"\n",
    "\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=50, fields=\"contentVector\", query_rewrites=\"generative|count-5\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"subtitle\", \"content\", \"publication\", \"responses\", \"claps\", \"date\"], \n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    query_rewrites=\"generative|count-5\",\n",
    "    query_language=\"en\",\n",
    "    debug=QueryDebugMode.QUERY_REWRITES,\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=3\n",
    ")\n",
    "\n",
    "text_query_rewrites = results.get_debug_info().query_rewrites.text.rewrites\n",
    "vector_query_rewrites = results.get_debug_info().query_rewrites.vectors[0].rewrites\n",
    "print_results(results)\n",
    "\n",
    "print(\"Text Query Rewrites:\")\n",
    "print(text_query_rewrites)\n",
    "print(\"Vector Query Rewrites:\")\n",
    "print(vector_query_rewrites)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
