{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector search in Python (Azure AI Search)\n",
    "\n",
    "This code demonstrates how to use Azure AI Search by using the push API to insert vectors into your search index:\n",
    "\n",
    "+ Create an index schema\n",
    "+ Load the sample data from a local folder\n",
    "+ Embed the documents in-memory using Azure OpenAI's text-embedding-3-large model\n",
    "+ Index the vector and nonvector fields on Azure AI Search\n",
    "+ Run a series of vector and hybrid queries, including metadata filtering and hybrid (text + vectors) search. \n",
    "\n",
    "The code uses Azure OpenAI to generate embeddings for title and content fields. You'll need access to Azure OpenAI to run this demo.\n",
    "\n",
    "The code reads the `articles_1000.json` file, which contains the input data for which embeddings need to be generated.\n",
    "\n",
    "The output is a combination of human-readable text and embeddings that can be pushed into a search index.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ An Azure subscription, with [access to Azure OpenAI](https://aka.ms/oai/access). You must have the Azure OpenAI service name and an API key.\n",
    "\n",
    "+ A deployment of the text-embedding-3-large embedding model.\n",
    "\n",
    "+ Azure AI Search, any tier, but choose a service that has sufficient capacity for your vector index. We recommend Basic or higher. [Enable semantic ranking](https://learn.microsoft.com/azure/search/semantic-how-to-enable-disable) if you want to run the hybrid query with semantic ranking.\n",
    "\n",
    "We used Python 3.11, [Visual Studio Code with the Python extension](https://code.visualstudio.com/docs/python/python-tutorial), and the [Jupyter extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) to test this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Python virtual environment in Visual Studio Code\n",
    "\n",
    "1. Open the Command Palette (Ctrl+Shift+P).\n",
    "1. Search for **Python: Create Environment**.\n",
    "1. Select **Venv**.\n",
    "1. Select a Python interpreter. Choose 3.10 or later.\n",
    "\n",
    "It can take a minute to set up. If you run into problems, see [Python environments in VS Code](https://code.visualstudio.com/docs/python/environments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r azure-search-vector-python-sample-requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (1.93.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lananoor\\onedrive - microsoft\\ai agents\\filteredqueryagent\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai pandas tqdm "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# The following variables from your .env file are used in this notebook\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\", \"\")) if len(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\", \"\")) > 0 else DefaultAzureCredential()\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX\", \"vectest\")\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\", \"\") if len(os.getenv(\"AZURE_OPENAI_KEY\", \"\")) > 0 else None\n",
    "azure_openai_embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-large\")\n",
    "azure_openai_embedding_dimensions = int(os.getenv(\"AZURE_OPENAI_EMBEDDING_DIMENSIONS\", 1024))\n",
    "embedding_model_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-large\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset as csv file \n",
    "df = pd.read_csv(r'C:\\Users\\lananoor\\OneDrive - Microsoft\\AI Agents\\FilteredQueryAgent\\ingestion\\medium_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://medium.datadriveninvestor.com/is-fasta...</td>\n",
       "      <td>Is FastAPI going to replace Django?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>226</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://medium.datadriveninvestor.com/whats-th...</td>\n",
       "      <td>What’s the Best Way to Buy a Reliable Luxury Car?</td>\n",
       "      <td>The full cost of ownership makes buying brand-...</td>\n",
       "      <td>186</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://medium.datadriveninvestor.com/credit-r...</td>\n",
       "      <td>Credit Risk Assessment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://medium.datadriveninvestor.com/cash-is-...</td>\n",
       "      <td>Cash is Trash or Cash is King? What´s it gonna...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://medium.datadriveninvestor.com/how-to-b...</td>\n",
       "      <td>How to be Flipin’ awesome for your fans</td>\n",
       "      <td>Flipboard magazines capitalize on marketing vi...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                url  \\\n",
       "0   1  https://medium.datadriveninvestor.com/is-fasta...   \n",
       "1   2  https://medium.datadriveninvestor.com/whats-th...   \n",
       "2   3  https://medium.datadriveninvestor.com/credit-r...   \n",
       "3   4  https://medium.datadriveninvestor.com/cash-is-...   \n",
       "4   5  https://medium.datadriveninvestor.com/how-to-b...   \n",
       "\n",
       "                                               title  \\\n",
       "0                Is FastAPI going to replace Django?   \n",
       "1  What’s the Best Way to Buy a Reliable Luxury Car?   \n",
       "2                             Credit Risk Assessment   \n",
       "3  Cash is Trash or Cash is King? What´s it gonna...   \n",
       "4            How to be Flipin’ awesome for your fans   \n",
       "\n",
       "                                            subtitle  claps  responses  \\\n",
       "0                                                NaN    226          4   \n",
       "1  The full cost of ownership makes buying brand-...    186          3   \n",
       "2                                                NaN     76          0   \n",
       "3                                                NaN    139          0   \n",
       "4  Flipboard magazines capitalize on marketing vi...     34          0   \n",
       "\n",
       "   reading_time           publication        date  \n",
       "0             4  Data Driven Investor  2020-05-24  \n",
       "1            10  Data Driven Investor  2020-05-24  \n",
       "2             7  Data Driven Investor  2020-05-24  \n",
       "3             6  Data Driven Investor  2020-05-24  \n",
       "4             5  Data Driven Investor  2020-05-24  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View data set \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11642"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any rows with null values\n",
    "df_no_nulls = df.dropna()\n",
    "\n",
    "# Select the first 1000 rows\n",
    "df_1000 = df_no_nulls.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://medium.datadriveninvestor.com/whats-th...</td>\n",
       "      <td>What’s the Best Way to Buy a Reliable Luxury Car?</td>\n",
       "      <td>The full cost of ownership makes buying brand-...</td>\n",
       "      <td>186</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://medium.datadriveninvestor.com/how-to-b...</td>\n",
       "      <td>How to be Flipin’ awesome for your fans</td>\n",
       "      <td>Flipboard magazines capitalize on marketing vi...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://medium.datadriveninvestor.com/biometri...</td>\n",
       "      <td>Biometrics for Authentication Security and Pri...</td>\n",
       "      <td>Biometrics is a growing field, and…</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.datadriveninvestor.com/what-mak...</td>\n",
       "      <td>What Makes Travel So Thrilling?</td>\n",
       "      <td>Photo by Rosa Diaz</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>https://medium.datadriveninvestor.com/lessons-...</td>\n",
       "      <td>Lessons Learned From Doing All The Productive ...</td>\n",
       "      <td>As advised by Google</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                                url  \\\n",
       "1    2  https://medium.datadriveninvestor.com/whats-th...   \n",
       "4    5  https://medium.datadriveninvestor.com/how-to-b...   \n",
       "6    7  https://medium.datadriveninvestor.com/biometri...   \n",
       "10  11  https://medium.datadriveninvestor.com/what-mak...   \n",
       "11  12  https://medium.datadriveninvestor.com/lessons-...   \n",
       "\n",
       "                                                title  \\\n",
       "1   What’s the Best Way to Buy a Reliable Luxury Car?   \n",
       "4             How to be Flipin’ awesome for your fans   \n",
       "6   Biometrics for Authentication Security and Pri...   \n",
       "10                    What Makes Travel So Thrilling?   \n",
       "11  Lessons Learned From Doing All The Productive ...   \n",
       "\n",
       "                                             subtitle  claps  responses  \\\n",
       "1   The full cost of ownership makes buying brand-...    186          3   \n",
       "4   Flipboard magazines capitalize on marketing vi...     34          0   \n",
       "6                 Biometrics is a growing field, and…    172          0   \n",
       "10                                 Photo by Rosa Diaz     93          0   \n",
       "11                               As advised by Google     87          0   \n",
       "\n",
       "    reading_time           publication        date  \n",
       "1             10  Data Driven Investor  2020-05-24  \n",
       "4              5  Data Driven Investor  2020-05-24  \n",
       "6              5  Data Driven Investor  2020-05-24  \n",
       "10             3  Data Driven Investor  2020-05-24  \n",
       "11             5  Data Driven Investor  2020-05-24  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1000.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new Content field using Azure OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is a city filled with iconic landmarks, world-class museums, charming neighborhoods, and delicious food. Here’s a list of must-see attractions and experiences to make the most of your trip:\n",
      "\n",
      "### **Iconic Landmarks**\n",
      "1. **Eiffel Tower**  \n",
      "   Visit the most recognizable landmark in Paris. You can admire it from the Trocadéro Gardens, picnic on the Champ de Mars, or go up to the top for stunning views of the city.\n",
      "\n",
      "2. **Louvre Museum**  \n",
      "   Home to masterpieces like the Mona Lisa and the Venus de Milo, the Louvre is a must-see for art lovers. Even if you don’t go inside, the glass pyramid is a sight to enjoy.\n",
      "\n",
      "3. **Notre-Dame Cathedral**  \n",
      "   Despite ongoing restoration after the 2019 fire, Notre-Dame remains a Gothic architectural masterpiece. Stroll around its exterior and explore Île de la Cité.\n",
      "\n",
      "4. **Sacré-Cœur Basilica**  \n",
      "   Located at the highest point in Paris, Montmartre, this white basilica offers incredible views of the city.\n",
      "\n",
      "5. **Arc de Triomphe and Champs-Élysées**  \n",
      "   Walk along the famous Champs-Élysées and visit the Arc de Triomphe for panoramic views. Don’t forget that you can climb to the top!\n",
      "\n",
      "6. **Palace of Versailles** (Day Trip)  \n",
      "   Venture just outside Paris to this opulent palace and walk through the enchanting Hall of Mirrors and sprawling gardens.\n",
      "\n",
      "---\n",
      "\n",
      "### **World-Class Museums**\n",
      "7. **Musée d'Orsay**  \n",
      "   Famous for its Impressionist and Post-Impressionist collections, including works by Monet, Van Gogh, and Renoir.\n",
      "\n",
      "8. **Centre Pompidou**  \n",
      "   A hub for modern and contemporary art with a rooftop view of Paris.\n",
      "\n",
      "9. **Musée de l'Orangerie**  \n",
      "   Known for Monet’s Water Lilies murals, this museum is a peaceful escape.\n",
      "\n",
      "10. **Rodin Museum**  \n",
      "    Explore Rodin’s sculptures, such as *The Thinker*, in a romantic garden setting.\n",
      "\n",
      "---\n",
      "\n",
      "### **Charming Neighborhoods**\n",
      "11. **Montmartre**  \n",
      "    Wander the cobblestone streets of this bohemian district, visiting the Place du Tertre and the Moulin Rouge.\n",
      "\n",
      "12. **Le Marais**  \n",
      "    A trendy area with historic architecture, boutique shops, and delicious falafel (try L’As du Fallafel!).\n",
      "\n",
      "13. **Saint-Germain-des-Prés**  \n",
      "    A chic district with famous cafés like Café de Flore and Les Deux Magots. Explore art galleries and bookstores here.\n",
      "\n",
      "14. **Latin Quarter**  \n",
      "    A vibrant area filled with narrow streets, bookshops (including the famous Shakespeare & Company), and the Panthéon.\n",
      "\n",
      "---\n",
      "\n",
      "### **Relaxing Spaces**\n",
      "15. **Jardin des Tuileries**  \n",
      "    A beautiful garden next to the Louvre, perfect for strolling or sitting.\n",
      "\n",
      "16. **Luxembourg Gardens**  \n",
      "    A tranquil spot to enjoy fountains, sculptures, and manicured lawns.\n",
      "\n",
      "17. **Parc des Buttes-Chaumont**  \n",
      "    A lesser-known park with a romantic vibe, featuring cliffs, waterfalls, and a view of Sacré-Cœur from a distance.\n",
      "\n",
      "---\n",
      "\n",
      "### **Historic Sites**\n",
      "18. **Conciergerie**  \n",
      "    Explore this medieval palace and prison where Marie Antoinette was held before her execution.\n",
      "\n",
      "19. **Pantheon**  \n",
      "    Honor legendary French figures such as Voltaire, Victor Hugo, and Marie Curie.\n",
      "\n",
      "---\n",
      "\n",
      "### **Food Experiences**\n",
      "20. **Parisian Cafés**  \n",
      "    Enjoy coffee and pastries at iconic cafés. Try a croissant at Angelina or a macaron at Ladurée.\n",
      "\n",
      "21. **Street Markets**  \n",
      "    Visit markets like Marché Bastille or Rue Cler for local produce, cheese, and bread.\n",
      "\n",
      "22. **Wine and Cheese Tasting**  \n",
      "    Indulge in French wines and artisanal cheeses.\n",
      "\n",
      "---\n",
      "\n",
      "### **Unique Experiences**\n",
      "23. **Seine River Cruise**  \n",
      "    See Paris from a different perspective on a boat tour that passes the Eiffel Tower, Notre-Dame, and other landmarks.\n",
      "\n",
      "24. **Catacombs of Paris**  \n",
      "    Venture underground to explore the ossuary that contains the remains of around six million people.\n",
      "\n",
      "25. **Opera Garnier**  \n",
      "    Marvel at this architectural gem and its lavish interiors.\n",
      "\n",
      "---\n",
      "\n",
      "### **Tips for Your Visit**\n",
      "- If you're visiting multiple attractions, consider purchasing a Paris Museum Pass for skip-the-line access.\n",
      "- Be prepared to walk a lot and wear comfortable shoes.\n",
      "- Don’t forget to take breaks to enjoy leisurely meals or coffee at the city's charming cafés.\n",
      "\n",
      "Enjoy your trip to Paris! Let me know if you'd like recommendations tailored to your specific interests. 😊\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Set up env variables \n",
    "endpoint = \"https://.openai.azure.com/\"\n",
    "model_name = \"gpt-4o\"\n",
    "deployment = \"gpt-4o\"\n",
    "\n",
    "subscription_key = \"\"\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I am going to Paris, what should I see?\",\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 119/1000 [03:00<28:23,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [24:45<00:00,  1.49s/it]\n",
      "C:\\Users\\lananoor\\AppData\\Local\\Temp\\ipykernel_44444\\3258349956.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_1000['content'] = contents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://medium.datadriveninvestor.com/whats-th...</td>\n",
       "      <td>What’s the Best Way to Buy a Reliable Luxury Car?</td>\n",
       "      <td>The full cost of ownership makes buying brand-...</td>\n",
       "      <td>186</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>The article explores the optimal approach to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://medium.datadriveninvestor.com/how-to-b...</td>\n",
       "      <td>How to be Flipin’ awesome for your fans</td>\n",
       "      <td>Flipboard magazines capitalize on marketing vi...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>The article explores strategies for creating e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://medium.datadriveninvestor.com/biometri...</td>\n",
       "      <td>Biometrics for Authentication Security and Pri...</td>\n",
       "      <td>Biometrics is a growing field, and…</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>The article explores the use of biometrics as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.datadriveninvestor.com/what-mak...</td>\n",
       "      <td>What Makes Travel So Thrilling?</td>\n",
       "      <td>Photo by Rosa Diaz</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>The article, titled \"What Makes Travel So Thri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>https://medium.datadriveninvestor.com/lessons-...</td>\n",
       "      <td>Lessons Learned From Doing All The Productive ...</td>\n",
       "      <td>As advised by Google</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Driven Investor</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>The article explores insights gained from impl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                                url  \\\n",
       "1    2  https://medium.datadriveninvestor.com/whats-th...   \n",
       "4    5  https://medium.datadriveninvestor.com/how-to-b...   \n",
       "6    7  https://medium.datadriveninvestor.com/biometri...   \n",
       "10  11  https://medium.datadriveninvestor.com/what-mak...   \n",
       "11  12  https://medium.datadriveninvestor.com/lessons-...   \n",
       "\n",
       "                                                title  \\\n",
       "1   What’s the Best Way to Buy a Reliable Luxury Car?   \n",
       "4             How to be Flipin’ awesome for your fans   \n",
       "6   Biometrics for Authentication Security and Pri...   \n",
       "10                    What Makes Travel So Thrilling?   \n",
       "11  Lessons Learned From Doing All The Productive ...   \n",
       "\n",
       "                                             subtitle  claps  responses  \\\n",
       "1   The full cost of ownership makes buying brand-...    186          3   \n",
       "4   Flipboard magazines capitalize on marketing vi...     34          0   \n",
       "6                 Biometrics is a growing field, and…    172          0   \n",
       "10                                 Photo by Rosa Diaz     93          0   \n",
       "11                               As advised by Google     87          0   \n",
       "\n",
       "    reading_time           publication        date  \\\n",
       "1             10  Data Driven Investor  2020-05-24   \n",
       "4              5  Data Driven Investor  2020-05-24   \n",
       "6              5  Data Driven Investor  2020-05-24   \n",
       "10             3  Data Driven Investor  2020-05-24   \n",
       "11             5  Data Driven Investor  2020-05-24   \n",
       "\n",
       "                                              content  \n",
       "1   The article explores the optimal approach to p...  \n",
       "4   The article explores strategies for creating e...  \n",
       "6   The article explores the use of biometrics as ...  \n",
       "10  The article, titled \"What Makes Travel So Thri...  \n",
       "11  The article explores insights gained from impl...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate summary for content field \n",
    "def generate_summary(title, subtitle):\n",
    "    prompt = (\n",
    "        f\"Given the following article information, generate a concise summary (about 200 words) of what the article is about. \"\n",
    "        f\"Base your summary ONLY on the title and subtitle. Do not add any extra information.\\n\\n\"\n",
    "        f\"Title: {title}\\nSubtitle: {subtitle}\\n\\nSummary:\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment,  # for Azure, use \"engine\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes articles based only on their title and subtitle.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=400,  # adjust if needed\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()  # <-- Corrected here\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Loop through the DataFrame and generate summaries\n",
    "contents = []\n",
    "for idx, row in tqdm(df_1000.iterrows(), total=len(df_1000)):\n",
    "    title = row['title']\n",
    "    subtitle = row['subtitle']\n",
    "    summary = generate_summary(title, subtitle)\n",
    "    contents.append(summary)\n",
    "\n",
    "# Add the new column\n",
    "df_1000['content'] = contents\n",
    "\n",
    "# Preview\n",
    "df_1000.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in 'content': 0\n"
     ]
    }
   ],
   "source": [
    "# Count the number of null values in the 'content' column\n",
    "null_count = df_1000['content'].isnull().sum()\n",
    "print(f\"Number of null values in 'content': {null_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert CSV to JSON fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to a list of dicts and write as JSON file\n",
    "df_1000.to_json(\"df_seperate_doc.json\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000.to_json(\"df_1000.json\", orient='records', lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries (one dict per row)\n",
    "records = df_1000.to_dict(orient='records')\n",
    "\n",
    "# Save as a JSON array (list of objects, one per row)\n",
    "with open(\"articles_1000.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id               int64\n",
      "url             object\n",
      "title           object\n",
      "subtitle        object\n",
      "claps            int64\n",
      "responses        int64\n",
      "reading_time     int64\n",
      "publication     object\n",
      "date            object\n",
      "content         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check Data Type \n",
    "print(df_1000.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add empty embedding field "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "Read your data, generate OpenAI embeddings and export to a format to insert your Azure AI Search index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Azure OpenAI config\n",
    "azure_openai_endpoint = \"https://.openai.azure.com/\"\n",
    "azure_openai_key = \"\"      \n",
    "azure_openai_embedding_deployment = \"text-embedding-3-large\"\n",
    "azure_openai_api_version = \"2023-05-15\"  \n",
    "embedding_model_name = azure_openai_embedding_deployment\n",
    "azure_openai_embedding_dimensions = 3072 \n",
    "\n",
    "client_embeddings = AzureOpenAI(\n",
    "    azure_deployment=azure_openai_embedding_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key\n",
    ")\n",
    "\n",
    "# Load your articles JSON (articles_1000.json)\n",
    "input_path = 'articles_1000.json'\n",
    "with open(input_path, 'r', encoding='utf-8') as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "# --- Prepare texts for embedding ---\n",
    "titles_text = [\n",
    "    (item.get('title', '') + '. ' + item.get('subtitle', '')).strip()\n",
    "    for item in input_data\n",
    "]\n",
    "content_text = [item.get('content', '') for item in input_data]\n",
    "\n",
    "# Optional: Replace empty strings with something (if you want to avoid blank embeddings)\n",
    "titles_text = [txt if txt.strip() else \"empty\" for txt in titles_text]\n",
    "content_text = [txt if txt.strip() else \"empty\" for txt in content_text]\n",
    "\n",
    "# --- Batch embedding function ---\n",
    "def batch(iterable, batch_size=16):\n",
    "    \"\"\"Yield successive batch_size-sized chunks from iterable.\"\"\"\n",
    "    for i in range(0, len(iterable), batch_size):\n",
    "        yield iterable[i:i + batch_size]\n",
    "\n",
    "# --- For titles_vector ---\n",
    "titles_embeddings = []\n",
    "for chunk in batch(titles_text, 16):\n",
    "    response = client_embeddings.embeddings.create(\n",
    "        input=chunk,\n",
    "        model=embedding_model_name,\n",
    "        dimensions=azure_openai_embedding_dimensions\n",
    "    )\n",
    "    titles_embeddings.extend([item.embedding for item in response.data])\n",
    "\n",
    "# --- For content_vector ---\n",
    "content_embeddings = []\n",
    "for chunk in batch(content_text, 16):\n",
    "    response = client_embeddings.embeddings.create(\n",
    "        input=chunk,\n",
    "        model=embedding_model_name,\n",
    "        dimensions=azure_openai_embedding_dimensions\n",
    "    )\n",
    "    content_embeddings.extend([item.embedding for item in response.data])\n",
    "\n",
    "# Assign embeddings to new fields in your documents\n",
    "for i, item in enumerate(input_data):\n",
    "    item['titlesVector'] = titles_embeddings[i]\n",
    "    item['contentVector'] = content_embeddings[i]\n",
    "\n",
    "# Output with new fields\n",
    "output_path = os.path.join('output', 'articles_final.json')\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(input_data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data Types in JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: {<class 'int'>}\n",
      "url: {<class 'str'>}\n",
      "title: {<class 'str'>}\n",
      "subtitle: {<class 'str'>}\n",
      "claps: {<class 'int'>}\n",
      "responses: {<class 'int'>}\n",
      "reading_time: {<class 'int'>}\n",
      "publication: {<class 'str'>}\n",
      "date: {<class 'str'>}\n",
      "content: {<class 'str'>}\n",
      "titlesVector: {<class 'list'>}\n",
      "contentVector: {<class 'list'>}\n"
     ]
    }
   ],
   "source": [
    "# Check data type of fields \n",
    "from collections import defaultdict\n",
    "\n",
    "types_per_field = defaultdict(set)\n",
    "\n",
    "for doc in data:\n",
    "    for k, v in doc.items():\n",
    "        types_per_field[k].add(type(v))\n",
    "\n",
    "for k, types in types_per_field.items():\n",
    "    print(f\"{k}: {types}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All IDs converted to strings.\n"
     ]
    }
   ],
   "source": [
    "# Convert id field from into to string \n",
    "import json\n",
    "\n",
    "# Load your JSON\n",
    "with open('output/articles_final.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert all 'id' fields to string\n",
    "for item in data:\n",
    "    item['id'] = str(item['id'])\n",
    "\n",
    "# Optionally, save it back\n",
    "with open('output/articles_final.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"All IDs converted to strings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 <class 'str'>\n",
      "2020-05-24 <class 'str'>\n",
      "2020-05-24 <class 'str'>\n",
      "2020-05-24 <class 'str'>\n",
      "2020-05-24 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Make sure date field is in the right format for DateTimeOffset in index \n",
    "\n",
    "with open('output/articles_final.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "for item in data[:5]:\n",
    "    print(item['date'], type(item['date']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: {<class 'str'>}\n",
      "url: {<class 'str'>}\n",
      "title: {<class 'str'>}\n",
      "subtitle: {<class 'str'>}\n",
      "claps: {<class 'int'>}\n",
      "responses: {<class 'int'>}\n",
      "reading_time: {<class 'int'>}\n",
      "publication: {<class 'str'>}\n",
      "date: {<class 'str'>}\n",
      "content: {<class 'str'>}\n",
      "titlesVector: {<class 'list'>}\n",
      "contentVector: {<class 'list'>}\n"
     ]
    }
   ],
   "source": [
    "# Final - Check data type of fields \n",
    "from collections import defaultdict\n",
    "\n",
    "types_per_field = defaultdict(set)\n",
    "\n",
    "for doc in data:\n",
    "    for k, v in doc.items():\n",
    "        types_per_field[k].add(type(v))\n",
    "\n",
    "for k, types in types_per_field.items():\n",
    "    print(f\"{k}: {types}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your search index\n",
    "\n",
    "Create your search index schema and vector search configuration. If you get an error, check the search service for available quota and check the .env file to make sure you're using a unique search index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://.search.windows.net\"\n",
    "api_key = \"\"\n",
    "credential = AzureKeyCredential(api_key)\n",
    "index_name = \"medium-articles-date-index\" # choose an index name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medium-articles-date-index created\n"
     ]
    }
   ],
   "source": [
    "index_name = \"medium-articles-date-index\"\n",
    "\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchFieldDataType,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters\n",
    ")\n",
    "\n",
    "# Set your actual endpoint, credential, and index_name\n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True, sortable=True, facetable=True),\n",
    "    SearchableField(name=\"url\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SearchableField(name=\"subtitle\", type=SearchFieldDataType.String),\n",
    "    SimpleField(name=\"claps\", type=SearchFieldDataType.Int32, filterable=True, sortable=True, facetable=True),\n",
    "    SimpleField(name=\"responses\", type=SearchFieldDataType.Int32, filterable=True, sortable=True, facetable=True),\n",
    "    SimpleField(name=\"reading_time\", type=SearchFieldDataType.Int32, filterable=True, sortable=True, facetable=True),\n",
    "    SearchableField(name=\"publication\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "    SimpleField(name=\"date\", type=SearchFieldDataType.DateTimeOffset, filterable=True, sortable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"titlesVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=3072, vector_search_profile_name=\"myHnswProfile\"),\n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=3072, vector_search_profile_name=\"myHnswProfile\"),\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "            vectorizer_name=\"myVectorizer\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            vectorizer_name=\"myVectorizer\",\n",
    "            parameters=AzureOpenAIVectorizerParameters(\n",
    "                resource_url=azure_openai_endpoint,\n",
    "                deployment_name=azure_openai_embedding_deployment,\n",
    "                model_name=embedding_model_name,\n",
    "                api_key=azure_openai_key\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        keywords_fields=[SemanticField(field_name=\"subtitle\"), SemanticField(field_name=\"publication\")],\n",
    "        content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search,\n",
    "    semantic_search=semantic_search\n",
    ")\n",
    "\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f'{result.name} created')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert text and embeddings into vector store\n",
    "Add texts and metadata from the JSON data to the vector store:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split JSON into smaller chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created articles_1.json with 10 docs.\n"
     ]
    }
   ],
   "source": [
    "# Split data into smaller batches \n",
    "import json\n",
    "\n",
    "input_path = r'C:\\Users\\lananoor\\OneDrive - Microsoft\\AI Agents\\FilteredQueryAgent\\ingestion\\output\\articles_final.json'\n",
    "output_path = r'C:\\Users\\lananoor\\OneDrive - Microsoft\\AI Agents\\FilteredQueryAgent\\ingestion\\output\\articles_1.json'\n",
    "\n",
    "# Read the full JSON file\n",
    "with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "    docs = json.load(infile)\n",
    "\n",
    "# Take the first 10 documents\n",
    "docs_10 = docs[:10]\n",
    "\n",
    "# Write them to the new file\n",
    "with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(docs_10, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Created articles_1.json with 10 docs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\\articles_1.json: 10 documents\n",
      "output\\articles_2.json: 90 documents\n",
      "output\\articles_3.json: 100 documents\n",
      "output\\articles_4.json: 200 documents\n",
      "output\\articles_5.json: 200 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for i in range(1, 6):\n",
    "    path = output_path = os.path.join('output', f'articles_{i}.json')\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            docs = json.load(f)\n",
    "            print(f\"{path}: {len(docs)} documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created articles_7.json with 200 docs.\n"
     ]
    }
   ],
   "source": [
    "# Split data into smaller batches \n",
    "import json\n",
    "\n",
    "input_path = r'C:\\Users\\lananoor\\OneDrive - Microsoft\\AI Agents\\FilteredQueryAgent\\ingestion\\output\\articles_final.json'\n",
    "output_path = r'C:\\Users\\lananoor\\OneDrive - Microsoft\\AI Agents\\FilteredQueryAgent\\ingestion\\output\\articles_7.json'\n",
    "\n",
    "# Read the full JSON file\n",
    "with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "    docs = json.load(infile)\n",
    "\n",
    "# Take the first 10 documents\n",
    "docs_10 = docs[800:1000]\n",
    "\n",
    "# Write them to the new file\n",
    "with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(docs_10, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Created articles_7.json with 200 docs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest JSON to AI Search Index in Batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "output_path = os.path.join('output', 'articles_1.json') \n",
    "\n",
    "# Upload some documents to the index  \n",
    "with open(output_path, 'r') as file:  \n",
    "    documents = json.load(file)  \n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)\n",
    "\n",
    "for res in result:\n",
    "    if not res.succeeded:\n",
    "        print(f\"Failed to upload doc id={res.key}, error: {res.error_message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 10 documents in total\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "\n",
    "output_path = os.path.join('output', 'articles_1.json') \n",
    "\n",
    "# Upload some documents to the index  \n",
    "with open(output_path, 'r', encoding='utf-8') as file:  \n",
    "    documents = json.load(file)  \n",
    "  \n",
    "# Use SearchIndexingBufferedSender to upload the documents in batches optimized for indexing  \n",
    "with SearchIndexingBufferedSender(  \n",
    "    endpoint=endpoint,  \n",
    "    index_name=index_name,  \n",
    "    credential=credential,  \n",
    ") as batch_client:  \n",
    "    # Add upload actions for all documents  \n",
    "    batch_client.upload_documents(documents=documents)  \n",
    "print(f\"Uploaded {len(documents)} documents in total\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 200 documents in total\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "\n",
    "output_path = os.path.join('output', 'articles_7.json') \n",
    "\n",
    "# Upload some documents to the index  \n",
    "with open(output_path, 'r', encoding='utf-8') as file:  \n",
    "    documents = json.load(file)  \n",
    "  \n",
    "# Use SearchIndexingBufferedSender to upload the documents in batches optimized for indexing  \n",
    "with SearchIndexingBufferedSender(  \n",
    "    endpoint=endpoint,  \n",
    "    index_name=index_name,  \n",
    "    credential=credential,  \n",
    ") as batch_client:  \n",
    "    # Add upload actions for all documents  \n",
    "    batch_client.upload_documents(documents=documents)  \n",
    "print(f\"Uploaded {len(documents)} documents in total\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Text Search  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 363\n",
      "Title: How to Overcome Your Phone Addiction With Mindfulness\n",
      "Publication: Better Humans\n",
      "Claps: 1300\n",
      "----------------------------------------\n",
      "ID: 497\n",
      "Title: How to Begin a Sugar-Free Life\n",
      "Publication: Better Humans\n",
      "Claps: 1200\n",
      "----------------------------------------\n",
      "ID: 743\n",
      "Title: Simple Changes That Helped Me Lose 5 Kg and Feel Great in Just One Month\n",
      "Publication: Better Humans\n",
      "Claps: 1400\n",
      "----------------------------------------\n",
      "ID: 857\n",
      "Title: How to Select the Best Exercises for Building Muscle\n",
      "Publication: Better Humans\n",
      "Claps: 523\n",
      "----------------------------------------\n",
      "ID: 974\n",
      "Title: Hardware to Boost Your Productivity in 2020\n",
      "Publication: Better Humans\n",
      "Claps: 2200\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Filter claps and publication \n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query using search.ismatch for Brodheadsville\n",
    "filter_query = \"publication eq 'Better Humans' and claps gt 500\"\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",  # Wildcard search to match all documents\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"publication\", \"claps\"],\n",
    "    top=5  # Limit to top 5 results\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Publication: {result['publication']}\")\n",
    "    print(f\"Claps: {result['claps']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 27\n",
      "Title: 10 eye-catching logo animations you’ll wish you made\n",
      "Publication: UX Collective\n",
      "Claps: 1700\n",
      "Responses: 10\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 30\n",
      "Title: How to stop the battle between Product Managers and Designers\n",
      "Publication: UX Collective\n",
      "Claps: 366\n",
      "Responses: 5\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 876\n",
      "Title: Loading: Neumorphism 2\n",
      "Publication: UX Collective\n",
      "Claps: 839\n",
      "Responses: 9\n",
      "Date: 2020-04-10T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 1202\n",
      "Title: 3D Design is in — my journey into trying 3D for the first time\n",
      "Publication: UX Collective\n",
      "Claps: 292\n",
      "Responses: 7\n",
      "Date: 2020-02-21T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 1096\n",
      "Title: Change in Google Search is killing it\n",
      "Publication: UX Collective\n",
      "Claps: 4700\n",
      "Responses: 46\n",
      "Date: 2020-02-16T00:00:00Z\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query\n",
    "filter_query = \"publication eq 'UX Collective' and responses ge 5\"\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",  # Wildcard search to match all documents\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"publication\", \"date\", \"claps\", \"responses\"],  # <-- fixed comma\n",
    "    top=5\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Publication: {result['publication']}\")\n",
    "    print(f\"Claps: {result['claps']}\")\n",
    "    print(f\"Responses: {result['responses']}\")\n",
    "    print(f\"Date: {result['date']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 27\n",
      "Title: 10 eye-catching logo animations you’ll wish you made\n",
      "Publication: UX Collective\n",
      "Responses: 10\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 30\n",
      "Title: How to stop the battle between Product Managers and Designers\n",
      "Publication: UX Collective\n",
      "Responses: 5\n",
      "Date: 2020-05-24T00:00:00Z\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Filter for publication, date and responses \n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query with correct DateTimeOffset literals\n",
    "filter_query = (\n",
    "    \"publication eq 'UX Collective' and \"\n",
    "    \"date ge 2020-05-01T00:00:00Z and \"\n",
    "    \"date le 2020-05-31T00:00:00Z and \"\n",
    "    \"responses ge 5\"\n",
    ")\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"publication\", \"date\", \"responses\"],\n",
    "    top=5\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Publication: {result['publication']}\")\n",
    "    print(f\"Responses: {result['responses']}\")\n",
    "    print(f\"Date: {result['date']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 389\n",
      "Title: I Lost $40,000 in a Month and Learned a Valuable Lesson\n",
      "Publication: The Startup\n",
      "Responses: 30\n",
      "Date: 2020-01-03T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 625\n",
      "Title: Stop Checking for Nulls\n",
      "Publication: The Startup\n",
      "Responses: 29\n",
      "Date: 2020-05-27T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 1208\n",
      "Title: Your Brain Is Not an Indestructible Punching Bag\n",
      "Publication: The Startup\n",
      "Responses: 26\n",
      "Date: 2020-02-21T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 386\n",
      "Title: Biggest Startup Failure in History\n",
      "Publication: The Startup\n",
      "Responses: 15\n",
      "Date: 2020-01-03T00:00:00Z\n",
      "----------------------------------------\n",
      "ID: 905\n",
      "Title: How to Fold The Deck Chairs on The Titanic\n",
      "Publication: The Startup\n",
      "Responses: 11\n",
      "Date: 2020-04-10T00:00:00Z\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Filter for publication, date and responses \n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query with correct DateTimeOffset literals\n",
    "filter_query = (\n",
    "    \"publication eq 'The Startup' and \"\n",
    "    \"date ge 2019-05-01T00:00:00Z and \"\n",
    "    \"date le 2021-05-31T00:00:00Z and \"\n",
    "    \"responses ge 10\"\n",
    ")\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"publication\", \"date\", \"responses\"],\n",
    "    top=5\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Publication: {result['publication']}\")\n",
    "    print(f\"Responses: {result['responses']}\")\n",
    "    print(f\"Date: {result['date']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1670\n",
      "Title: Google Algorithm Update What You Need to Know\n",
      "Subtitle: Remaining up-to-date with…\n",
      "Publication: Data Driven Investor\n",
      "Reading Time: 7\n",
      "----------------------------------------\n",
      "ID: 114\n",
      "Title: How to Set Up and Track Goals in Google Analytics\n",
      "Subtitle: Understanding if your marketing and content is…\n",
      "Publication: The Startup\n",
      "Reading Time: 6\n",
      "----------------------------------------\n",
      "ID: 984\n",
      "Title: How to A/B Test With Google Optimize: A Complete Guide\n",
      "Subtitle: Setup, install, test, and analyze multiple…\n",
      "Publication: Better Marketing\n",
      "Reading Time: 12\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query\n",
    "filter_query = \"search.ismatch('Google', 'title') and reading_time gt 5\"\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",  # Wildcard search to match all documents\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"subtitle\", \"publication\", \"reading_time\"],  # <-- fixed comma\n",
    "    top=5\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Subtitle: {result['subtitle']}\")\n",
    "    print(f\"Publication: {result['publication']}\")\n",
    "    print(f\"Reading Time: {result['reading_time']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 158\n",
      "Title: Who Rules the Cloud Service: AWS or Azure?\n",
      "Subtitle: Comparison Between Amazon Web Service Vs Microsoft Azure\n",
      "Content: The article explores the competition between Amazon Web Services (AWS) and Microsoft Azure, two leading cloud service providers. It presents a comparison of the features, capabilities, and offerings of each platform to determine which dominates the cloud service industry.\n",
      "Reading Time: 10\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Define the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Define the filter query\n",
    "filter_query = \"search.ismatch('Microsoft', 'content') and reading_time gt 5\"\n",
    "\n",
    "# Perform the search query with the filter, limiting to top 5 results\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",  # Wildcard search to match all documents\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"subtitle\", \"content\", \"reading_time\"],  # <-- fixed comma\n",
    "    top=5\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Subtitle: {result['subtitle']}\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(f\"Reading Time: {result['reading_time']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 230\n",
      "Title: Introduction to Azure Cache for Redis with .NET Core\n",
      "Subtitle: Azure Cache for Redis provides us with a powerful…\n",
      "Content: The article titled \"Introduction to Azure Cache for Redis with .NET Core\" explores the capabilities of Azure Cache for Redis and its integration with .NET Core applications. The subtitle suggests that Azure Cache for Redis is a powerful tool, likely emphasizing its utility in enhancing application performance and scalability. The article likely provides an overview of how developers can leverage this caching service within their .NET Core projects to optimize data retrieval and processing.\n",
      "Reading Time: 9\n",
      "Responses: 0\n",
      "Claps: 96\n",
      "----------------------------------------\n",
      "ID: 158\n",
      "Title: Who Rules the Cloud Service: AWS or Azure?\n",
      "Subtitle: Comparison Between Amazon Web Service Vs Microsoft Azure\n",
      "Content: The article explores the competition between Amazon Web Services (AWS) and Microsoft Azure, two leading cloud service providers. It presents a comparison of the features, capabilities, and offerings of each platform to determine which dominates the cloud service industry.\n",
      "Reading Time: 10\n",
      "Responses: 0\n",
      "Claps: 60\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Example filter:\n",
    "# - \"Azure\" appears in content\n",
    "# - reading_time more than 7\n",
    "# - responses more than 3\n",
    "# - claps at least 500\n",
    "\n",
    "filter_query = (\n",
    "    \"search.ismatch('Azure', 'content') \"\n",
    "    \"and reading_time gt 7 \"\n",
    "    \"and responses lt 3 \"\n",
    "    \"and claps lt 500\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",  # Wildcard search\n",
    "    filter=filter_query,\n",
    "    select=[\"id\", \"title\", \"subtitle\", \"content\", \"reading_time\", \"responses\", \"claps\"],  # typo fixed\n",
    "    top=5\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Subtitle: {result['subtitle']}\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(f\"Reading Time: {result['reading_time']}\")\n",
    "    print(f\"Responses: {result['responses']}\")\n",
    "    print(f\"Claps: {result['claps']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
